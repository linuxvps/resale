import logging
import os

import networkx as nx
import pandas as pd
from colorlog import ColoredFormatter
from sklearn.compose import ColumnTransformer
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.tree import DecisionTreeClassifier
# ------------------------------------------------------------
# Ø¨Ø®Ø´ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ SQLAlchemy Ø¨Ø±Ø§ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ùˆ ØªØ¹Ø±ÛŒÙ Ø§Ù†ØªÛŒØªÛŒ
# ------------------------------------------------------------
from sqlalchemy import create_engine
from sqlalchemy.orm import declarative_base, sessionmaker

Base = declarative_base()
protected_columns = ['LOAN_AMOUNT', 'CURRENT_LOAN_RATES']

formatter = ColoredFormatter("%(log_color)s%(asctime)s - %(levelname)s - %(message)s", datefmt=None, reset=True,
                             log_colors={'DEBUG': 'cyan', 'INFO': 'white', 'WARNING': 'yellow', 'ERROR': 'red',
                                         'CRITICAL': 'bold_red', })

handler = logging.StreamHandler()
handler.setFormatter(formatter)

logger = logging.getLogger()
logger.addHandler(handler)
logger.setLevel(logging.INFO)

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)
pd.set_option('display.float_format', lambda x: '%.2f' % x)

###########################################
# Ù¾Ù„Ø§Øª
###########################################
import matplotlib.pyplot as plt
import numpy as np
from typing import Tuple
import seaborn as sns


class Plot:
    """
    ÛŒÚ© Ú©Ù„Ø§Ø³ Ø¹Ù…ÙˆÙ…ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨ØµØ±ÛŒâ€ŒØ³Ø§Ø²ÛŒ ØªÙˆØ²ÛŒØ¹ Ø§Ø­ØªÙ…Ø§Ù„â€ŒÙ‡Ø§.
    """

    def __init__(self) -> None:
        pass

    def plot_default_prob_hist(self,           # â† Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù† self
                               probs,
                               u, v,
                               bins=100,
                               figsize=(12, 6),
                               log_y=True,
                               title='Distribution of Default Probabilities with Thresholds (u, v)'):
        """
        Ø±Ø³Ù… Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… Ø§Ø­ØªÙ…Ø§Ù„ Ù†Ú©ÙˆÙ„ Ø¨Ù‡â€ŒÙ‡Ù…Ø±Ø§Ù‡ Ø®Ø·ÙˆØ· Ø¢Ø³ØªØ§Ù†Ù‡Ù” u Ùˆ v
        """
        plt.figure(figsize=figsize)

        n, bins_edges, _ = plt.hist(probs,
                                    bins=bins,
                                    color='skyblue',
                                    edgecolor='black',
                                    alpha=0.7)

        mean_val = np.mean(probs)
        plt.axvline(mean_val, color='red', linestyle='--', label=f'Mean = {mean_val:.2f}')
        plt.axvline(u,        color='green',  linewidth=2, label=f'u (POS) = {u:.3f}')
        plt.axvline(v,        color='orange', linewidth=2, label=f'v (NEG) = {v:.3f}')

        if log_y:
            plt.yscale('log')

        plt.title(title)
        plt.xlabel('Probability')
        plt.ylabel('Frequency' + (' (log scale)' if log_y else ''))
        plt.legend()
        plt.grid(True, linestyle='--', alpha=0.5)
        plt.tight_layout()
        plt.show()

    def draw_preprocessing_flowchart(self, output_path="flowchart_standardize_select.png"):
        G = nx.DiGraph()

        G.add_node("Extract Data\n(Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§)")
        G.add_node("Data Cleaning\n(Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§)")
        G.add_node("Convert to Standard Format\n(ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ù‚Ø§Ù„Ø¨ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯)")
        G.add_node("Handle Missing & Invalid Values\n(Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù†Ø§Ù‚Øµ/Ù†Ø§ØµØ­ÛŒØ­)")
        G.add_node("Standardize Numeric & Date Columns\n(Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ùˆ ØªØ§Ø±ÛŒØ®ÛŒ)")
        G.add_node("Correlation Analysis\n(ØªØ­Ù„ÛŒÙ„ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ)")
        G.add_node("Remove Redundant Features\n(Ø­Ø°Ù ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ)")
        G.add_node("Feature Selection\n(Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ)")
        G.add_node("Preprocessed Data Ready for Modeling\n(Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ)")

        G.add_edges_from([
            ("Extract Data\n(Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§)", "Data Cleaning\n(Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§)"),
            ("Data Cleaning\n(Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§)", "Convert to Standard Format\n(ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ù‚Ø§Ù„Ø¨ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯)"),
            ("Convert to Standard Format\n(ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ù‚Ø§Ù„Ø¨ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯)",
             "Handle Missing & Invalid Values\n(Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù†Ø§Ù‚Øµ/Ù†Ø§ØµØ­ÛŒØ­)"),
            ("Handle Missing & Invalid Values\n(Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù†Ø§Ù‚Øµ/Ù†Ø§ØµØ­ÛŒØ­)",
             "Standardize Numeric & Date Columns\n(Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ùˆ ØªØ§Ø±ÛŒØ®ÛŒ)"),
            ("Standardize Numeric & Date Columns\n(Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ùˆ ØªØ§Ø±ÛŒØ®ÛŒ)",
             "Correlation Analysis\n(ØªØ­Ù„ÛŒÙ„ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ)"),
            ("Correlation Analysis\n(ØªØ­Ù„ÛŒÙ„ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ)", "Remove Redundant Features\n(Ø­Ø°Ù ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ)"),
            ("Remove Redundant Features\n(Ø­Ø°Ù ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ)", "Feature Selection\n(Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ)"),
            ("Feature Selection\n(Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ)",
             "Preprocessed Data Ready for Modeling\n(Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ)")
        ])

        plt.figure(figsize=(12, 8))
        pos = nx.spring_layout(G, seed=42)
        nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=3500, font_size=10,
                font_weight='bold', arrows=True, arrowstyle='->', arrowsize=20)
        plt.title("Flowchart: Standardization & Feature Selection Process", fontsize=14, fontweight='bold')

        plt.savefig(output_path, dpi=300, bbox_inches="tight")
        plt.show()

    def plot1(self, probabilities: np.ndarray, bins: int = 100, figsize: Tuple[int, int] = (10, 6),
              xlim: Tuple[float, float] = None) -> None:
        """
        Ù†Ù…ÙˆØ¯Ø§Ø± Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… ØªÙˆØ²ÛŒØ¹ Ø§Ø­ØªÙ…Ø§Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¢Ø±Ø§ÛŒÙ‡ ÙˆØ±ÙˆØ¯ÛŒ Ø±Ø³Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        Ø¨Ø§ Ø§ÙØ²ÙˆØ¯Ù† Ø¬Ø²Ø¦ÛŒØ§Øª Ù…Ø§Ù†Ù†Ø¯ Ø®Ø· Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†ØŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚ Ù…Ø­ÙˆØ± Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª grid.

        :param probabilities: Ø¢Ø±Ø§ÛŒÙ‡ numpy Ø´Ø§Ù…Ù„ Ø§Ø­ØªÙ…Ø§Ù„â€ŒÙ‡Ø§.
        :param bins: ØªØ¹Ø¯Ø§Ø¯ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… (Ù¾ÛŒØ´â€ŒÙØ±Ø¶ 100).
        :param figsize: Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø´Ú©Ù„ Ù†Ù…ÙˆØ¯Ø§Ø± (Ù¾ÛŒØ´â€ŒÙØ±Ø¶ (10, 6)).
        :param xlim: Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ø­ÙˆØ± Ø§ÙÙ‚ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª (min, max). Ø§Ú¯Ø± None Ø¨Ø§Ø´Ø¯ØŒ Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± ØªÙ†Ø¸ÛŒÙ… Ù…ÛŒâ€ŒØ´ÙˆØ¯.
        """
        plt.figure(figsize=figsize)

        # Ø±Ø³Ù… Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù…
        n, bins, patches = plt.hist(probabilities, bins=bins, edgecolor='black', alpha=0.7, color='skyblue')

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ Ù†Ù…Ø§ÛŒØ´ Ø®Ø· Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†
        mean_val = np.mean(probabilities)
        plt.axvline(mean_val, color='red', linestyle='dashed', linewidth=2, label=f'Mean = {mean_val:.2f}')

        # ØªÙ†Ø¸ÛŒÙ… Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ø­ÙˆØ± Ø§ÙÙ‚ÛŒ Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²
        if xlim is not None:
            plt.xlim(xlim)

        # Ø§ÙØ²ÙˆØ¯Ù† Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø±
        plt.title("Distribution of Default Probabilities", fontsize=16)
        plt.xlabel("Probability", fontsize=14)
        plt.ylabel("Frequency", fontsize=14)
        plt.xticks(fontsize=12)
        plt.yticks(fontsize=12)
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.legend(fontsize=12)
        plt.tight_layout()
        plt.show()

    def plot_label_count(self, label_counts: pd.Series) -> None:
        plt.figure(figsize=(10, 6))

        # ØªØ¨Ø¯ÛŒÙ„ Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¹Ø¯Ø¯ÛŒ
        label_counts.index = label_counts.index.astype(int)

        # ØªØ¨Ø¯ÛŒÙ„ Ø³Ø±ÛŒ Ø¨Ù‡ DataFrame Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² hue
        label_df = pd.DataFrame({'Labels': label_counts.index, 'Frequency': label_counts.values})

        sns.barplot(x='Labels', y='Frequency', data=label_df, hue='Labels', dodge=False, palette=['#4CAF50', '#FF6F61'],
                    legend=False)

        plt.title('Label Distribution After Conversion', fontsize=18)
        plt.xlabel('Labels (0: Non-Default, 1: Default)', fontsize=14)
        plt.ylabel('Frequency', fontsize=14)
        plt.xticks(ticks=[0, 1], labels=['Non-Default (0)', 'Default (1)'])
        plt.grid(axis='y', linestyle='--', alpha=0.6)
        plt.tight_layout()
        plt.show()

    def plot_with_thresholds(self,
                             probabilities: np.ndarray,
                             u: float,            # â† Ù‚Ø¨Ù„Ø§Ù‹ alpha Ø¨ÙˆØ¯
                             v: float,            # â† Ù‚Ø¨Ù„Ø§Ù‹ beta Ø¨ÙˆØ¯
                             bins: int = 100,
                             figsize: Tuple[int, int] = (12, 6),
                             xlim: Tuple[float, float] = None) -> None:
        """
        Ø±Ø³Ù… Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… Ø§Ø­ØªÙ…Ø§Ù„ Ù†Ú©ÙˆÙ„ Ø¨Ù‡â€ŒÙ‡Ù…Ø±Ø§Ù‡ Ø®Ø·ÙˆØ· u Ùˆ v (Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡Ù” Ø¬Ù‡Ø§Ù†ÛŒ).
        """
        plt.figure(figsize=figsize)

        # Û±) Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… Ø§Ø­ØªÙ…Ø§Ù„â€ŒÙ‡Ø§
        n, bins_array, patches = plt.hist(probabilities,
                                          bins=bins,
                                          edgecolor='black',
                                          alpha=0.7,
                                          color='skyblue')

        # Û²) Ø®Ø· Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†
        mean_val = np.mean(probabilities)
        plt.axvline(mean_val,
                    color='red',
                    linestyle='dashed',
                    linewidth=2,
                    label=f'Mean = {mean_val:.2f}')

        # Û³) Ø®Ø·ÙˆØ· u Ùˆ v  (Ø¨Ù‡â€ŒØªØ±ØªÛŒØ¨ ØªØµÙ…ÛŒÙ…Ù POS Ùˆ NEGÙ Ø³Ø±Ø§Ø³Ø±ÛŒ)
        plt.axvline(u,
                    color='green',
                    linestyle='-',
                    linewidth=3,
                    label=f'u (POS) = {u:.3f}')
        plt.axvline(v,
                    color='orange',
                    linestyle='-',
                    linewidth=3,
                    label=f'v (NEG) = {v:.3f}')

        # Ø¬Ø²Ø¦ÛŒØ§Øª Ù†Ù…ÙˆØ¯Ø§Ø±
        plt.title("Distribution of Default Probabilities with Global Thresholds (u, v)",
                  fontsize=16)
        plt.xlabel("Probability", fontsize=14)
        plt.ylabel("Frequency", fontsize=14)
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.legend(fontsize=12)

        if xlim:
            plt.xlim(xlim)

        plt.tight_layout()
        plt.show()

    def plot_feature_importance(self, model, feature_names, top_n=20):
        importance = model.feature_importances_
        feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importance})
        feature_importance = feature_importance.sort_values(by='Importance', ascending=False).head(top_n)

        plt.figure(figsize=(14, 12))  # Ø§ÙØ²Ø§ÛŒØ´ Ø§Ø±ØªÙØ§Ø¹ Ù†Ù…ÙˆØ¯Ø§Ø±

        sns.barplot(
            x='Importance',
            y='Feature',
            data=feature_importance,
            palette='viridis',
            hue='Feature',
            dodge=False
        )

        plt.title('Feature Importance (Top {})'.format(top_n), fontsize=16)
        plt.xlabel('Importance', fontsize=14)
        plt.ylabel('Features', fontsize=14)
        plt.yticks(fontsize=10)  # Ú©Ø§Ù‡Ø´ Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙÙˆÙ†Øª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
        plt.legend([], [], frameon=False)  # Ø­Ø°Ù legend Ø§Ø¶Ø§ÙÛŒ
        plt.tight_layout()  # ØªÙ†Ø¸ÛŒÙ… Ø®ÙˆØ¯Ú©Ø§Ø± Ø­Ø§Ø´ÛŒÙ‡â€ŒÙ‡Ø§
        plt.show()


    def plot_pca(self, X: pd.DataFrame, n_components: int = 2):
        """
        Ø§Ø¬Ø±Ø§ÛŒ PCA Ùˆ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø±ØµØ¯ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ù…Ø¤Ù„ÙÙ‡â€ŒÙ‡Ø§
        :param X: Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø¨Ù‡ Ø´Ú©Ù„ DataFrame)
        :param n_components: ØªØ¹Ø¯Ø§Ø¯ Ù…Ø¤Ù„ÙÙ‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´
        """
        pca = PCA(n_components=n_components)
        X_pca = pca.fit_transform(X)

        explained_variance = pca.explained_variance_ratio_
        cumulative_variance = explained_variance.cumsum()

        plt.figure(figsize=(10, 6))
        plt.bar(range(1, n_components + 1), explained_variance * 100, alpha=0.5, align='center',
                label='Individual Explained Variance')
        plt.step(range(1, n_components + 1), cumulative_variance * 100, where='mid',
                 label='Cumulative Explained Variance')
        plt.xlabel('Principal Components', fontsize=14)
        plt.ylabel('Percentage of Variance Explained', fontsize=14)
        plt.title('Explained Variance by Principal Components', fontsize=16)
        plt.legend(loc='best')
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.show()

        print(f"Ù…Ù‚Ø¯Ø§Ø± ÙˆØ§Ø±ÛŒØ§Ù†Ø³ ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ù…Ø¤Ù„ÙÙ‡â€ŒÙ‡Ø§: {explained_variance}")
        print(f"ÙˆØ§Ø±ÛŒØ§Ù†Ø³ ØªØ¬Ù…Ø¹ÛŒ (Cumulative Variance): {cumulative_variance[-1]}")

    def explained_variance(self, x_train, n_components=10):
        pca = PCA(n_components=n_components)
        pca.fit(x_train)
        explained_variance_ratio = pca.explained_variance_ratio_
        cumulative_variance = np.cumsum(explained_variance_ratio)

        # Ù†Ù…Ø§ÛŒØ´ Ù†Ø³Ø¨Øª ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ùˆ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ ØªØ¬Ù…Ø¹ÛŒ
        print(f"Explained Variance Ratio: {explained_variance_ratio}")
        print(f"Cumulative Variance: {cumulative_variance}")

        # Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±
        plt.figure(figsize=(10, 6))
        plt.bar(range(1, n_components + 1), explained_variance_ratio * 100, alpha=0.6, color='skyblue',
                label='Individual Explained Variance')
        plt.plot(range(1, n_components + 1), cumulative_variance * 100, color='blue', marker='o',
                 label='Cumulative Explained Variance')
        plt.title('Explained Variance by Principal Components')
        plt.xlabel('Principal Components')
        plt.ylabel('Percentage of Variance Explained')
        plt.legend()
        plt.grid(True)
        plt.show()

    def plot_pca_2d(self, x_train):
        pca = PCA(n_components=2)
        x_pca = pca.fit_transform(x_train)
        plt.figure(figsize=(10, 6))
        plt.scatter(x_pca[:, 0], x_pca[:, 1], c='blue', alpha=0.5)
        plt.title('2D PCA Plot')
        plt.xlabel('Principal Component 1')
        plt.ylabel('Principal Component 2')
        plt.grid(True)
        plt.show()

    def plot_pca_3d(self, x_train):
        pca = PCA(n_components=3)
        x_pca = pca.fit_transform(x_train)

        fig = plt.figure(figsize=(10, 6))
        ax = fig.add_subplot(111, projection='3d')
        ax.scatter(x_pca[:, 0], x_pca[:, 1], x_pca[:, 2], c='blue', alpha=0.5)
        ax.set_title('3D PCA Plot')
        ax.set_xlabel('Principal Component 1')
        ax.set_ylabel('Principal Component 2')
        ax.set_zlabel('Principal Component 3')
        plt.show()

    def plot_tsne(self, x_train):
        tsne = TSNE(n_components=2, random_state=42)
        x_tsne = tsne.fit_transform(x_train)

        plt.figure(figsize=(10, 6))
        plt.scatter(x_tsne[:, 0], x_tsne[:, 1], c='blue', alpha=0.5)
        plt.title('t-SNE Plot')
        plt.xlabel('t-SNE Dimension 1')
        plt.ylabel('t-SNE Dimension 2')
        plt.grid(True)
        plt.show()

    def plot_pareto_front(self, front_costs):
        """
        Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆØ¯Ø§Ø± Ù¾Ø§Ø±ØªÙˆ Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒ NSGA-II (cost vs boundary size)
        """
        plt.figure(figsize=(8, 6))
        plt.scatter(front_costs[:, 0], front_costs[:, 1], c='red', label='Pareto Front')
        plt.xlabel('Total Cost', fontsize=14)
        plt.ylabel('Boundary Size', fontsize=14)
        plt.title('Pareto Front - NSGA-II', fontsize=16)
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.legend()
        plt.tight_layout()
        plt.show()

    def plot_confusion_matrix(self, y_true, y_pred):
        """
        Ø±Ø³Ù… Ù…Ø§ØªØ±ÛŒØ³ Ø³Ø±Ø¯Ø±Ú¯Ù…ÛŒ Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ (Confusion Matrix)
        """
        cm = confusion_matrix(y_true, y_pred)
        labels = ['NEG', 'POS']
        plt.figure(figsize=(6, 5))
        sns.heatmap(cm, annot=True, fmt='d', cmap="Blues", xticklabels=labels, yticklabels=labels)
        plt.title("Confusion Matrix - Final Model", fontsize=14)
        plt.xlabel("Predicted Label", fontsize=12)
        plt.ylabel("True Label", fontsize=12)
        plt.tight_layout()
        plt.show()


from sqlalchemy import Column, BigInteger, Integer, Numeric, DateTime, Date, String, CHAR, Float
from datetime import datetime


class ParsianLoan(Base):
    __tablename__ = "parsian_loan_2"

    id = Column(BigInteger, primary_key=True, autoincrement=True)
    advance_pay = Column(Numeric(28, 8), nullable=True)
    advance_pay_to_remain_non_cash = Column(Numeric(28, 8), nullable=True)
    advance_pay_to_total_cash = Column(Numeric(28, 8), nullable=True)
    approval_amount = Column(Numeric(28, 8), nullable=True)
    bank_share_cash_amount = Column(Numeric(28, 8), nullable=True)
    bank_share_non_cash_amount = Column(Numeric(28, 8), nullable=True)
    branch_code = Column(Integer, nullable=False)
    branchname = Column(String(100, collation='utf8mb4_unicode_ci'), nullable=True)
    charge = Column(Numeric(28, 8), nullable=True)
    loan_file_numberr = Column(BigInteger, nullable=True)
    client_id = Column(Integer, nullable=True)
    commission_amount_remain = Column(Numeric(28, 8), nullable=True)
    contract = Column(String(255, collation='utf8mb4_unicode_ci'), nullable=True)
    create_date = Column(Date, nullable=True)
    customer_obligation_amount = Column(Numeric(28, 8), nullable=True)
    customer_share_cash_amount = Column(Numeric(28, 8), nullable=True)
    customer_share_non_cash_amount = Column(Numeric(28, 8), nullable=True)
    discount = Column(Numeric(28, 8), nullable=True)
    due_date = Column(Date, nullable=True)
    finalized_loan_amount = Column(Numeric(28, 8), nullable=True)
    first_over_due = Column(Date, nullable=True)
    first_passed = Column(Date, nullable=True)
    first_payment_date_in_du = Column(Date, nullable=True)
    frequency = Column(Integer, nullable=True)
    inc_commission_amount = Column(Numeric(28, 8), nullable=True)
    insert_sysdate = Column(DateTime(6), nullable=False, default=datetime.utcnow)
    installment_number_remain = Column(Integer, nullable=True)
    interest_amount = Column(Numeric(28, 8), nullable=True)
    interest_rate = Column(Numeric(19, 2), nullable=True)
    interest_sum = Column(Numeric(28, 8), nullable=True)
    is_installment = Column(CHAR, nullable=True)
    loan_duration_day = Column(Integer, nullable=True)
    loan_file_number = Column(BigInteger, nullable=True)
    long_title = Column(String(255, collation='utf8mb4_unicode_ci'), nullable=True)
    obligation_penalty = Column(Numeric(28, 8), nullable=True)
    passed_date = Column(Date, nullable=True)
    penalty = Column(Numeric(28, 8), nullable=True)
    penalty_interest = Column(Numeric(28, 8), nullable=True)
    principal_sum = Column(Numeric(28, 8), nullable=True)
    receivable_installment_number = Column(Integer, nullable=True)
    sit_distribute_phases = Column(Integer, nullable=True)
    sit_duration = Column(Integer, nullable=True)
    sit_duration_day = Column(Integer, nullable=True)
    sit_fast_receive_percent = Column(Float, nullable=True)
    sit_flag = Column(CHAR, nullable=True)
    status = Column(String(255, collation='utf8mb4_unicode_ci'), nullable=True)
    title = Column(String(255, collation='utf8mb4_unicode_ci'), nullable=True)
    to_due_date = Column(Numeric(28, 8), nullable=True)
    to_end_of_month = Column(Numeric(28, 8), nullable=True)
    total_payment_up_to_now = Column(Numeric(28, 8), nullable=True)
    total_repayment_up_to_now = Column(Numeric(28, 8), nullable=True)

    def __repr__(self):
        return f"<ParsianLoan(id={self.id})>"


class LoanDetail(Base):
    __tablename__ = "MY_TABLE"

    ID = Column(BigInteger, primary_key=True, autoincrement=False)
    LOAN_FILE_NUMBER = Column(BigInteger, nullable=True)
    LOAN_AMOUNT = Column(Numeric(65, 2), nullable=True)
    TOTAL_DEBT_IN_TOMAN = Column(Numeric(65, 2), nullable=True)
    CURRENT_LOAN_RATES = Column(Numeric(65, 2), nullable=True)
    LOAN_PURPOSE = Column(String(255), nullable=True)
    CONTRACT_DUE_DATE = Column(Date, nullable=True)
    INSTALLMENT_LOAN_AWARD_DATE = Column(Date, nullable=True)
    FIRST_PAYMENT_DATE_IN_DU = Column(Date, nullable=True)
    GRANT_DATE = Column(Date, nullable=True)
    APPLICATION_TYPE = Column(CHAR, nullable=True)
    LOAN_STATUS = Column(String(255), nullable=True)
    TOTAL_INSTALLMENT_AMOUNT = Column(Numeric(65, 2), nullable=True)
    NUM_OF_INSTALLMENTS = Column(BigInteger, nullable=True)
    FIRST_INSTALLMENT_DUE = Column(Date, nullable=True)
    LAST_INSTALLMENT_DUE = Column(Date, nullable=True)
    DEFAULT_COUNT = Column(BigInteger, nullable=True)
    COMPANY_TYPE = Column(BigInteger, nullable=True)
    POSTAL_CODE = Column(String(20), nullable=True)
    CITY_CODE = Column(String(20), nullable=True)
    REGION = Column(String(255), nullable=True)
    PROVINCE = Column(String(255), nullable=True)
    APPROXIMATE_INCOME_IN_TOMAN = Column(Numeric(65, 2), nullable=True)
    ANNUAL_TURNOVER_IN_TOMAN = Column(Numeric(65, 2), nullable=True)


class LoanRepository:
    """
    Ø±ÛŒÙ¾Ø§Ø²ÛŒØªÙˆØ±ÛŒ Ø¨Ø±Ø§ÛŒ ÙˆØ§Ú©Ø´ÛŒ Ø¯Ø§Ø¯Ù‡ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³ parsian_loan Ø¨Ø§ SQLAlchemy.
    """

    def __init__(self):
        # ÙØ±Ø¶ Ú©Ù†ÛŒØ¯ Connection String Ø±Ø§ Ø¯Ø± Ù…ØªØºÛŒØ±Ù…Ø­ÛŒØ·ÛŒ DB_CONNECTION_STRING Ù…ÛŒâ€ŒÚ¯Ø°Ø§Ø±ÛŒÙ….
        self.db_conn_str = os.getenv("DB_CONNECTION_STRING", "mysql+pymysql://root:pass@localhost:3306/ln")
        self.engine = create_engine(self.db_conn_str)
        SessionLocal = sessionmaker(bind=self.engine)
        self.session = SessionLocal()

    def fetch_loans_in_chunks(self, excluded_columns,chunk_size=100000):
        total_rows = self.session.query(LoanDetail).count()
        offset = 0
        dataframes = []
        while offset < total_rows:
            loans_chunk = (self.session.query(LoanDetail)
                           .order_by(LoanDetail.LOAN_FILE_NUMBER.desc())
                           .offset(offset)
                           .limit(chunk_size)
                           .all())
            if not loans_chunk:
                break
            # Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø¯Ø§Ø±ÛŒÙ…
            all_columns = list(LoanDetail.__table__.columns.keys())
            selected_columns = [col for col in all_columns if col not in excluded_columns]
            data = {col: [getattr(loan, col) for loan in loans_chunk] for col in selected_columns}
            df_chunk = pd.DataFrame(data)
            dataframes.append(df_chunk)
            offset += chunk_size
            logging.info(f"Ø¯Ø±ÛŒØ§ÙØª {min(offset, total_rows)} Ø§Ø² {total_rows} Ø±Ú©ÙˆØ±Ø¯")
        if dataframes:
            return pd.concat(dataframes, ignore_index=True)
        else:
            return pd.DataFrame()



    def fetch_loans(self,excluded_columns, limit=10_000):
        """
        ÙˆØ§Ú©Ø´ÛŒ Ø­Ø¯Ø§Ú©Ø«Ø± `limit` Ø±Ú©ÙˆØ±Ø¯ Ø§Ø² Ø¬Ø¯ÙˆÙ„ parsian_loan.
        Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ù‚Ø§Ù„Ø¨ ÛŒÚ© DataFrame Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.
        """
        # Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª ØªÙ…Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¬Ø¯ÙˆÙ„
        all_columns = [column.name for column in LoanDetail.__table__.columns]
        # Ø§Ù†ØªØ®Ø§Ø¨ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ù„ÛŒØ³Øª excluded ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ù†Ø¯
        selected_columns = [col for col in all_columns if col not in excluded_columns]

        # Ø§Ø¬Ø±Ø§ÛŒ Ú©ÙˆØ¦Ø±ÛŒ Ø¨Ø§ Ø§Ù†ØªØ®Ø§Ø¨ ÙÙ‚Ø· Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±

        loans = (self.session.query(*[getattr(LoanDetail, col) for col in selected_columns]).order_by(
            LoanDetail.LOAN_FILE_NUMBER.desc()).limit(limit).all())

        if not loans:
            logging.warning("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            return pd.DataFrame()

        # ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ DataFrame
        data = {col: [getattr(loan, col) for loan in loans] for col in selected_columns}
        df = pd.DataFrame(data)
        logging.info(f"âœ… {len(df)} Ø±Ú©ÙˆØ±Ø¯ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯ (LoanDetail).")
        return df


###########################################
# Ú¯Ø§Ù… Ø¯ÙˆÙ…: Ø¨Ø±Ø¢ÙˆØ±Ø¯ Ø§Ø­ØªÙ…Ø§Ù„ Ù†Ú©ÙˆÙ„ (Default Probability)
###########################################

class ParsianDefaultProbabilityModel:
    """
    ÛŒÚ© Ú©Ù„Ø§Ø³ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú¯Ø§Ù… Ø¯ÙˆÙ… Ø¯Ø± pseudocodeorg:
    - Ø¢Ù…ÙˆØ²Ø´ ÛŒÚ© Ù…Ø¯Ù„ (Ù…Ø«Ù„Ø§Ù‹ LightGBM) Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
    - Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ Ù†Ú©ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…ÙˆÙ†
    """

    def __init__(self, model_type="lightgbm", n_estimators=100, learning_rate=0.05, random_state=42, **kwargs):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
          - model_type: Ù†ÙˆØ¹ Ù…Ø¯Ù„ (lightgbm, xgboost ÛŒØ§ Ù‡Ø± Ù…Ø¯Ù„ Ø¯ÛŒÚ¯Ø±ÛŒ)
          - n_estimators, learning_rate: Ù‡Ø§ÛŒÙ¾Ø±Ø§Ø³ØªØ¹Ø¯Ø§Ø¯ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„
          - random_state: Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²ØªÙˆÙ„ÛŒØ¯ Ù¾Ø°ÛŒØ±ÛŒ
          - kwargs: Ø³Ø§ÛŒØ± Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¯Ù„Ø®ÙˆØ§Ù‡ Ù…Ø¯Ù„
        """

        self.model_type = model_type
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.random_state = random_state
        self.model = None
        self.default_probabilities_ = None  # Ø§Ø­ØªÙ…Ø§Ù„ Ù†Ú©ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ test

        self.kwargs = kwargs

    def fit_model(self, x_train, y_train):
        """
        Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ø§Ù†ØªØ®Ø§Ø¨ÛŒ:
        Ø¯Ø± Ø§ÛŒÙ† Ù…Ø«Ø§Ù„ØŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ LightGBM Ø§Ø³Øª.
        """
        if self.model_type.lower() == "lightgbm":
            logging.info("ğŸ”µ Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ LightGBM...")
            self.model = LGBMClassifier(n_estimators=self.n_estimators, learning_rate=self.learning_rate,
                                        random_state=self.random_state, **self.kwargs)
        else:
            raise ValueError("ÙØ¹Ù„Ø§Ù‹ ÙÙ‚Ø· Ù…Ø¯Ù„ lightgbm Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯ (Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„).")

        self.model.fit(x_train, y_train)
        Plot().plot_feature_importance(default_model.model, x_train.columns)

        logging.info("âœ… Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.")

    def predict_default_probability(self, x_test):
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ Ù†Ú©ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª.
        Ø®Ø±ÙˆØ¬ÛŒ: ÛŒÚ© Ø¢Ø±Ø§ÛŒÙ‡ NumPy Ø§Ø² Ù…Ù‚Ø§Ø¯ÛŒØ± Ø§Ø­ØªÙ…Ø§Ù„ (Ø¨ÛŒÙ† Û° Ùˆ Û±)
        """
        if not self.model:
            raise ValueError("Ø§Ø¨ØªØ¯Ø§ Ø¨Ø§ÛŒØ¯ Ù…Ø¯Ù„ Ø±Ø§ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ (fit_model).")

        logging.info("ğŸ”µ Ø¯Ø± Ø­Ø§Ù„ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ Ù†Ú©ÙˆÙ„ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…ÙˆÙ†...")
        probs = self.model.predict_proba(x_test)
        # Ú†ÙˆÙ† Ø®Ø±ÙˆØ¬ÛŒ predict_proba Ø¯Ùˆ Ø³ØªÙˆÙ† [Prob_of_Class_0, Prob_of_Class_1] Ø§Ø³ØªØŒ
        # Ø§Ø­ØªÙ…Ø§Ù„ Ù†Ú©ÙˆÙ„ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø³ØªÙˆÙ† Ø¯ÙˆÙ… (Ú©Ù„Ø§Ø³ Û±) Ø§Ø³Øª.
        self.default_probabilities_ = probs[:, 1]
        return self.default_probabilities_

    def get_model(self):
        """ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø´ÛŒØ¡ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡. """
        return self.model


# ------------------------------------------------------------
# Ú¯Ø§Ù… Ø§ÙˆÙ„: Preprocessing Manager Ùˆ Preprocessor
# ------------------------------------------------------------
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import RFECV
from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler


class LoanPreprocessor:
    """
    Ú©Ù„Ø§Ø³ Ù…Ø³Ø¦ÙˆÙ„ Ø¹Ù…Ù„ÛŒØ§Øª Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´:
    - ØªØ¨Ø¯ÛŒÙ„ Ø¨Ø±Ú†Ø³Ø¨ (status) Ø¨Ù‡ Ù†Ú©ÙˆÙ„/ØºÛŒØ±Ù†Ú©ÙˆÙ„ (Û° ÛŒØ§ Û±)
    - ØªØ¨Ø¯ÛŒÙ„ Ø§Ù†ÙˆØ§Ø¹ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¹Ø¯Ø¯ÛŒ ÛŒØ§ Ø²Ù…Ø§Ù†
    - Ø­Ø°Ù ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ø§Ù„Ø§
    - Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø§ Ú©Ù…Ú© LGBM + RFECV
    - Ø§ÛŒÙ…Ù¾ÛŒÙˆØª Ø¯Ø§Ø¯Ù‡ (SimpleImputer)
    """

    def __init__(self, imputation_strategy="mean"):
        self.imputer = SimpleImputer(strategy=imputation_strategy)
        self.scaler = StandardScaler()  # Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø«Ø§Ø¨Øª

    def standardize_numeric_columns(self, df: pd.DataFrame, exclude_cols: list = None,
                                    fit: bool = True) -> pd.DataFrame:
        """
        Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø±Ø§ Ø¨Ù‡ Ú©Ù…Ú© Zâ€‘score Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        :param df: Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… ÙˆØ±ÙˆØ¯ÛŒ
        :param exclude_cols: Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†Ø¨Ø§ÛŒØ¯ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø´ÙˆÙ†Ø¯ (Ù…Ø«Ù„ Ø¨Ø±Ú†Ø³Ø¨)
        :param fit: Ø§Ú¯Ø± True Ø¨Ø§Ø´Ø¯ØŒ scaler Ø±Ø§ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ df Ù…ÛŒâ€ŒÙÛŒØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯Ø›
                    Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±Øª ÙÙ‚Ø· transform Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
        :return: Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø´Ø¯Ù‡
        """
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if exclude_cols:
            numeric_cols = [c for c in numeric_cols if c not in exclude_cols]
        if fit:
            df[numeric_cols] = self.scaler.fit_transform(df[numeric_cols])
        else:
            df[numeric_cols] = self.scaler.transform(df[numeric_cols])
        return df


    def convert_labels(self, df, label_column="status"):
        logging.info(f"[LoanPreprocessor] ØªØ¨Ø¯ÛŒÙ„ Ø¨Ø±Ú†Ø³Ø¨: {label_column}")
        if label_column not in df.columns:
            raise ValueError(f"Ø³ØªÙˆÙ† {label_column} Ø¯Ø± Ø¯Ø§Ø¯Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")

        logger.warning(df[label_column].value_counts())

        # ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± {"Ù…Ø´ÙƒÙˆÙƒ Ø§Ù„ÙˆØµÙˆÙ„", "Ù…Ø¹ÙˆÙ‚", "Ø³Ø±Ø±Ø³ÙŠØ¯ Ú¯Ø°Ø´ØªÙ‡"} => 1
        default_statuses = {"Ù…Ø´ÙƒÙˆÙƒ Ø§Ù„ÙˆØµÙˆÙ„", "Ù…Ø¹ÙˆÙ‚", "Ø³Ø±Ø±Ø³ÙŠØ¯ Ú¯Ø°Ø´ØªÙ‡", "Ø³Ø±Ø±Ø³ÙŠØ¯" , "Ø¨Ø§Ø·Ù„ Ø´Ø¯Ù‡" , "Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø±Ø¯ Ø´Ø¯"}
        df[label_column] = df[label_column].apply(lambda x: 1 if x in default_statuses else 0)
        # Ù„Ø§Ú¯ Ú¯Ø±ÙØªÙ† Ø§Ø² ØªÙˆØ²ÛŒØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        label_counts = df[label_column].value_counts()
        logging.warning(label_counts.to_string())

        Plot().plot_label_count(label_counts)
        return df

    def convert_dataframe_columns(self, df):
        """
        ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ® Ø¨Ù‡ timestamp Ùˆ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ object Ø¨Ù‡ numeric.
        Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±ØªØŒ Ø§Ø² LabelEncoder Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
        """
        for col in df.columns:
            if np.issubdtype(df[col].dtype, np.datetime64):
                df[col] = pd.to_datetime(df[col]).apply(lambda x: x.timestamp() if pd.notnull(x) else np.nan)
            elif df[col].dtype == 'object':
                try:
                    df[col] = pd.to_numeric(df[col])
                except Exception:
                    le = LabelEncoder()
                    df[col] = le.fit_transform(df[col].astype(str))
        return df

    def remove_highly_correlated_features(self, data, threshold=0.9, class_column=None):
        new_data = data.copy()
        numeric_cols = new_data.select_dtypes(include=[np.number]).columns.tolist()
        if class_column and class_column in numeric_cols:
            numeric_cols.remove(class_column)

        protected_columns = ["approval_amount", "interest_amount"]  # Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†Ø¨Ø§ÛŒØ¯ Ø­Ø°Ù Ø´ÙˆÙ†Ø¯
        numeric_cols = [c for c in numeric_cols if c not in protected_columns]

        corr_matrix = new_data[numeric_cols].corr()
        attributes_to_remove = set()
        for i in range(len(numeric_cols) - 1):
            for j in range(i + 1, len(numeric_cols)):
                col_i = numeric_cols[i]
                col_j = numeric_cols[j]
                corr_value = corr_matrix.loc[col_i, col_j]
                if abs(corr_value) > threshold:
                    logging.info(f"Ø­Ø°Ù ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø§ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ø§Ù„Ø§: {col_i} - {col_j} => {corr_value:.2f}, Ø­Ø°Ù {col_j}")
                    attributes_to_remove.add(col_j)

        for col_r in attributes_to_remove:
            if col_r in new_data.columns:
                new_data.drop(columns=[col_r], inplace=True)

        return new_data

    def select_features(self, X, y):
        lgbm_estimator = LGBMClassifier(n_estimators=100, learning_rate=0.05, random_state=42, verbose=-1)
        rfecv = RFECV(estimator=lgbm_estimator, step=1, cv=5, scoring='accuracy', n_jobs=-1, verbose=0)
        rfecv.fit(X, y)
        selected_features = list(X.columns[rfecv.support_])
        for col in protected_columns:
            if col in X.columns and col not in selected_features:
                selected_features.append(col)
        not_selected_features = [col for col in X.columns if col not in selected_features]
        logging.info("ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡: " + ", ".join(selected_features))
        logging.info("ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø´Ø¯Ù‡: " + ", ".join(not_selected_features))
        return X.loc[:, selected_features]

    def summary_stats_for_df(self,df: pd.DataFrame) -> pd.DataFrame:
        """
        Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø³ØªÙˆÙ† Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± DataFrameØŒ Ø¢Ù…Ø§Ø±ÛŒ Ø´Ø§Ù…Ù„:
        ØªØ¹Ø¯Ø§Ø¯ ÛŒÚ©ØªØ§ØŒ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ù…Ø´Ø¯Ù‡ (NaN)ØŒ Ù…ÛŒÙ†ÛŒÙ…ÙˆÙ…ØŒ Ù…Ø§Ú©Ø³ÛŒÙ…ÙˆÙ…ØŒ Ø¯Ø§Ù…Ù†Ù‡ (Ù…Ø§Ú©Ø³ÛŒÙ…ÙˆÙ… - Ù…ÛŒÙ†ÛŒÙ…ÙˆÙ…)ØŒ
        Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†ØŒ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ùˆ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø± Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

        Ù†Ú©Ø§Øª:
        - Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒØŒ ØªÙ…Ø§Ù…ÛŒ Ø¢Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ù‚ÛŒÙ‚ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.
        - Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ® (datetime)ØŒ Ù…ÛŒÙ†ÛŒÙ…ÙˆÙ…ØŒ Ù…Ø§Ú©Ø³ÛŒÙ…ÙˆÙ…ØŒ Ø¯Ø§Ù…Ù†Ù‡ Ùˆ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† (Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªØ§Ø±ÛŒØ® Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†) Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
          Ø§Ù…Ø§ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ùˆ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø± Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø¹Ø¯Ù… ØªÙ†Ø§Ø³Ø¨ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨Ø§ ØªØ§Ø±ÛŒØ®ØŒ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† None Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.
        - Ø¨Ø±Ø§ÛŒ Ø³Ø§ÛŒØ± Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ØŒ ØªÙ†Ù‡Ø§ Ù…ÛŒÙ†ÛŒÙ…ÙˆÙ… Ùˆ Ù…Ø§Ú©Ø³ÛŒÙ…ÙˆÙ… (Ùˆ Ø¯Ø± ØµÙˆØ±Øª Ø§Ù…Ú©Ø§Ù† Ø¯Ø§Ù…Ù†Ù‡) Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡ Ùˆ Ø¢Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø±Ø§Ø¨Ø± None Ø®ÙˆØ§Ù‡Ù†Ø¯ Ø¨ÙˆØ¯.
        """
        stats_rows = []

        for col in df.columns:
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ ÛŒÚ©ØªØ§ (Ø¨Ø¯ÙˆÙ† Ø§Ø­ØªØ³Ø§Ø¨ NaN)
            unique_count = df[col].nunique(dropna=True)
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
            missing_count = df[col].isna().sum()

            # Ø¨Ø±Ø±Ø³ÛŒ Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡ Ø³ØªÙˆÙ† Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø§Ø³Ø¨
            try:
                # Ø§Ú¯Ø± Ø³ØªÙˆÙ† Ø¹Ø¯Ø¯ÛŒ Ø¨Ø§Ø´Ø¯
                if np.issubdtype(df[col].dtype, np.number):
                    col_min = df[col].min(skipna=True)
                    col_max = df[col].max(skipna=True)
                    col_range = col_max - col_min if (col_min is not None and col_max is not None) else None
                    col_mean = df[col].mean(skipna=True)
                    col_var = df[col].var(skipna=True)
                    col_std = df[col].std(skipna=True)

                # Ø§Ú¯Ø± Ø³ØªÙˆÙ† Ø§Ø² Ù†ÙˆØ¹ ØªØ§Ø±ÛŒØ® (datetime) Ø¨Ø§Ø´Ø¯
                elif np.issubdtype(df[col].dtype, np.datetime64):
                    col_min = df[col].min(skipna=True)
                    col_max = df[col].max(skipna=True)
                    # Ø¯Ø§Ù…Ù†Ù‡ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø§Ø®ØªÙ„Ø§Ù Ø²Ù…Ø§Ù† Ø¨ÛŒÙ† Ø¨ÛŒØ´ÛŒÙ†Ù‡ Ùˆ Ú©Ù…ÛŒÙ†Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
                    col_range = col_max - col_min if (col_min is not None and col_max is not None) else None
                    # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØ§Ø±ÛŒØ® Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯Ø› ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ùˆ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø± Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† None ØªØ¹ÛŒÛŒÙ† Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
                    col_mean = df[col].mean(skipna=True)
                    col_var = None
                    col_std = None

                # Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ØºÛŒØ± Ø¹Ø¯Ø¯ÛŒ Ùˆ ØºÛŒØ± ØªØ§Ø±ÛŒØ®
                else:
                    col_min = df[col].min()
                    col_max = df[col].max()
                    # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…ÛŒÙ†ÛŒÙ…ÙˆÙ… Ùˆ Ù…Ø§Ú©Ø³ÛŒÙ…ÙˆÙ… Ù‚Ø§Ø¨Ù„ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø³ØªÙ†Ø¯ Ùˆ Ø¯Ø± ØµÙˆØ±Øª Ø§Ù…Ú©Ø§Ù† Ø¯Ø§Ù…Ù†Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
                    try:
                        col_range = col_max - col_min
                    except Exception:
                        col_range = None
                    col_mean, col_var, col_std = None, None, None

            except Exception as e:
                col_min, col_max, col_range, col_mean, col_var, col_std = None, None, None, None, None, None

            stats_rows.append({
                "Ù…ØªØºÛŒØ±": col,
                "ØªØ¹Ø¯Ø§Ø¯ ÛŒÚ©ØªØ§": unique_count,
                "Ú¯Ù…Ø´Ø¯Ù‡": missing_count,
                "Ù…ÛŒÙ†ÛŒÙ…ÙˆÙ…": col_min,
                "Ù…Ø§Ú©Ø³ÛŒÙ…ÙˆÙ…": col_max,
                "Ø¯Ø§Ù…Ù†Ù‡": col_range,
                "Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†": col_mean,
                "ÙˆØ§Ø±ÛŒØ§Ù†Ø³": col_var,
                "Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±": col_std
            })

        stats_df = pd.DataFrame(stats_rows)
        return stats_df


class ParsianPreprocessingManager:
    """
    ÛŒÚ© Ú©Ù„Ø§Ø³ Ø¬Ø§Ù…Ø¹ Ø¨Ø±Ø§ÛŒ Ú¯Ø§Ù… Ø§ÙˆÙ„ (Preprocessing) Ø§Ø² pseudocodeorg.
    """

    def __init__(self, repository, limit_records=10000, label_column="status", imputation_strategy="mean",
                 need_2_remove_highly_correlated_features=False, correlation_threshold=0.9, do_balance=True,
                 test_size=0.2, random_state=42):
        self.repository = repository
        self.limit_records = limit_records
        self.label_column = label_column
        self.imputation_strategy = imputation_strategy
        self.correlation_threshold = correlation_threshold
        self.need_2_remove_highly_correlated_features = need_2_remove_highly_correlated_features
        self.do_balance = do_balance
        self.test_size = test_size
        self.random_state = random_state

        self.x_train = None
        self.y_train = None
        self.x_test = None
        self.y_test = None
        self.original_df = None
        self.preprocessor = None

    def step1_process_data(self):
        """
        1) ÙˆØ§Ú©Ø´ÛŒ Ø¯Ø§Ø¯Ù‡ Ø§Ø² parsian_loan
        2) ØªØ¨Ø¯ÛŒÙ„ Ø¨Ø±Ú†Ø³Ø¨ Ù†Ú©ÙˆÙ„ => 0/1
        3) ØªØ¨Ø¯ÛŒÙ„ Ø§Ù†ÙˆØ§Ø¹ Ø³ØªÙˆÙ† => Ø¹Ø¯Ø¯ÛŒ
        4) Ø­Ø°Ù ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ø§Ù„Ø§
        5) Ø§ÛŒÙ…Ù¾ÛŒÙˆØª
        6) ØªÙÚ©ÛŒÚ© X,y
        7) train_test_split
        8) Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒ
        9) Ù…ØªØ¹Ø§Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ SMOTE (Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²)

        Ø®Ø±ÙˆØ¬ÛŒ: (x_train, y_train, x_test, y_test, original_df)
        """
        logging.info("ğŸ”µ [Step1] Ø´Ø±ÙˆØ¹ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Preprocessing).")
        excluded_columns = [LoanDetail.REGION.key,LoanDetail.ID.key,LoanDetail.COMPANY_TYPE.key]
        # Ø§Ú¯Ø± ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ Ø¨Ø³ÛŒØ§Ø± Ø²ÛŒØ§Ø¯ Ø¨Ø§Ø´Ø¯ØŒ Ø§Ø² Ø±ÙˆØ´ chunk Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        if self.limit_records > 50_000:
            df = self.repository.fetch_loans_in_chunks(excluded_columns,chunk_size=100000)
        else:
            df = self.repository.fetch_loans(excluded_columns,limit=self.limit_records)

        if df.empty:
            logging.error("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯. ÙØ±Ø¢ÛŒÙ†Ø¯ Ù¾Ø§ÛŒØ§Ù† ÛŒØ§ÙØª.")
            return None, None, None, None, None
        logging.info(f"ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯ Ø¯Ø±ÛŒØ§ÙØªÛŒ: {len(df)}")

        self.preprocessor = LoanPreprocessor(imputation_strategy=self.imputation_strategy)

        # Ø¨Ø±Ú†Ø³Ø¨ Ù†Ú©ÙˆÙ„
        df = self.preprocessor.convert_labels(df, label_column=self.label_column)

        # ØªØ¨Ø¯ÛŒÙ„ Ø§Ù†ÙˆØ§Ø¹ Ø³ØªÙˆÙ†
        df = self.preprocessor.convert_dataframe_columns(df)

        # Ù…Ø«Ø§Ù„ Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù Ø¨Ø±Ø®ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
        drop_columns = ["create_date"]  # Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² Ù…ÛŒâ€ŒØªÙˆØ§Ù† ØªØºÛŒÛŒØ± Ø¯Ø§Ø¯
        for col_d in drop_columns:
            if col_d in df.columns:
                df.drop(columns=[col_d], inplace=True, errors="ignore")

        # Ø­Ø°Ù ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ø§Ù„Ø§
        if self.need_2_remove_highly_correlated_features:
            df = self.preprocessor.remove_highly_correlated_features(df, threshold=self.correlation_threshold,
                                                                     class_column=self.label_column)

        summary_stats_for_df = self.preprocessor.summary_stats_for_df(df)
        logging.error(summary_stats_for_df)
        # Ø§ÛŒÙ…Ù¾ÛŒÙˆØª
        # Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù‡Ù…Ù‡ Ù…Ù‚Ø¯Ø§Ø±Ø´ÙˆÙ† NaN Ù‡Ø³Øª
        all_nan_cols = df.columns[df.isna().all()].tolist()
        if all_nan_cols:
            logging.warning(f"Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù‡Ù…Ù‡ NaN: {all_nan_cols}")
            df.drop(columns=all_nan_cols, inplace=True)

        df_imputed = pd.DataFrame(self.preprocessor.imputer.fit_transform(df), columns=df.columns)

        X = df_imputed.drop(columns=[self.label_column])
        y = df_imputed[self.label_column].astype(int)

        X = self.preprocessor.standardize_numeric_columns(X, exclude_cols=[self.label_column], fit=True)

        # ØªÙÚ©ÛŒÚ© X,y
        X = df_imputed.drop(columns=[self.label_column])
        y = df_imputed[self.label_column]

        # ØªÙ‚Ø³ÛŒÙ… Ø¢Ù…ÙˆØ²Ø´/Ø¢Ø²Ù…ÙˆÙ†
        x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size,
                                                            random_state=self.random_state)

        # Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒ
        x_train_selected = self.preprocessor.select_features(x_train, y_train)
        x_test_selected = x_test[x_train_selected.columns]

        # Ù…ØªØ¹Ø§Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ SMOTE
        if self.do_balance:
            logging.info("ğŸ”µ Ø§Ø¹Ù…Ø§Ù„ SMOTE Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ù„Ø§Ù†Ø³ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§")
            x_train_bal, y_train_bal = SMOTE(random_state=self.random_state).fit_resample(x_train_selected, y_train)
        else:
            logging.info("ğŸ”µ Ø¹Ø¯Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² SMOTE")
            x_train_bal, y_train_bal = x_train_selected, y_train

        # Ø°Ø®ÛŒØ±Ù‡ Ø®Ø±ÙˆØ¬ÛŒ
        self.x_train = x_train_bal
        self.y_train = y_train_bal
        self.x_test = x_test_selected
        self.y_test = y_test
        self.original_df = df.copy()

        logging.info("âœ… [Step1] Ù…Ø±Ø­Ù„Ù‡ Ø§ÙˆÙ„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.")
        return (self.x_train, self.y_train, self.x_test, self.y_test, self.original_df)


###########################################
# Ú¯Ø§Ù… Ø³ÙˆÙ…: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø§ØªØ±ÛŒØ³ Ø²ÛŒØ§Ù† (loss Computation)
###########################################
class ParsianLossMatrix:
    """
    Ø¯Ø± Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ØŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø±ÛŒØ§Ù† Ù†Ù‚Ø¯ÛŒ (Ù…Ø«Ù„Ø§Ù‹ approval_amount, interest_amount)
    Ù‡Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†Ù…ÙˆÙ†Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

    ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… dataframe ØªØ³Øª (x_test) Ø±Ø§ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ… Ú©Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ Ø¯Ø± Ø¢Ù† ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯.
    Ù…Ø«Ù„Ø§Ù‹ approval_amount, interest_amount.

    Ø³Ù¾Ø³ Ú†Ù‡Ø§Ø± Ù†ÙˆØ¹ Ø²ÛŒØ§Ù† Ø§ØµÙ„ÛŒ Ø±Ø§ ØªØ¹Ø±ÛŒÙ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:
      Î»_PP, Î»_NN (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ 0)
      Î»_PN (Ø²ÛŒØ§Ù† Ù¾Ø°ÛŒØ±Ø´ Ø§Ø´ØªØ¨Ø§Ù‡)
      Î»_NP (Ø²ÛŒØ§Ù† Ø±Ø¯ Ø§Ø´ØªØ¨Ø§Ù‡)

    Ú©Ø§Ø±Ø¨Ø± Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø³ØªÙ‡ Ø¨Ù‡ Ù†ÛŒØ§Ø²Ø´ Ø§ÛŒÙ† ÙØ±Ù…ÙˆÙ„â€ŒÙ‡Ø§ Ø±Ø§ Ø´Ø®ØµÛŒâ€ŒØ³Ø§Ø²ÛŒ Ú©Ù†Ø¯.
    """

    def __init__(self, df_test: pd.DataFrame, approval_col="approval_amount", interest_col="interest_amount"):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
          - df_test: Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…ÙˆÙ†ØŒ Ø´Ø§Ù…Ù„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ Ù„Ø§Ø²Ù…
          - approval_col: Ù†Ø§Ù… Ø³ØªÙˆÙ†ÛŒ Ú©Ù‡ Ø§ØµÙ„ ÙˆØ§Ù… Ø±Ø§ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±Ø¯ (approval_amount)
          - interest_col: Ù†Ø§Ù… Ø³ØªÙˆÙ†ÛŒ Ú©Ù‡ Ø³ÙˆØ¯ Ø¨Ø§Ù„Ù‚ÙˆÙ‡ ÛŒØ§ Ù…Ø¨Ù„Øº Ø¨Ù‡Ø±Ù‡ Ø±Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ (interest_amount)
        """
        self.df_test = df_test.reset_index(drop=True)  # Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø§Ø³Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø±ÛŒØ³Øª Ø´ÙˆØ¯
        self.approval_col = approval_col
        self.interest_col = interest_col

        # Ø¯Ø± Ø§ÛŒÙ† Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù‡Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØµÙ…ÛŒÙ… Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†Ù…ÙˆÙ†Ù‡ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±ÛŒÙ…
        # Ù…Ø«Ø§Ù„: cost_matrix[i] = {
        #    "PP": val, "PN": val, "BP": val, "BN": val, "NP": val, "NN": val
        # }
        self.cost_matrix = []

    def compute_costs(self):
        """
        Î»_PP = Î»_NN = 0
        Î»_PN = interest          (Ø²ÛŒØ§Ù† Ù¾Ø°ÛŒØ±Ø´ Ø§Ø´ØªØ¨Ø§Ù‡Ù ØºÛŒØ±Ù†Ú©ÙˆÙ„)
        Î»_NP = principal + interest   (Ø²ÛŒØ§Ù† Ø±Ø¯Ù Ø§Ø´ØªØ¨Ø§Ù‡Ù Ù†Ú©ÙˆÙ„)
        Î»_BP = uÂ·Î»_NP , Î»_BN = vÂ·Î»_PN  â‡ Ø¯Ø§Ø®Ù„ NSGA-II Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        """
        if self.approval_col not in self.df_test.columns or self.interest_col not in self.df_test.columns:
            raise ValueError("Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø²ÛŒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª.")

        self.cost_matrix.clear()
        for i in range(len(self.df_test)):
            principal = float(self.df_test.loc[i, self.approval_col] or 0.0)
            interest = float(self.df_test.loc[i, self.interest_col] or 0.0)

            self.cost_matrix.append({
                "PP": 0.0,
                "NN": 0.0,
                "PN": interest,
                "NP": principal + interest  # Ù†Ù‡ Ø¶Ø±Ø¨!  Ø¬Ù…Ø¹ Ø·Ø¨Ù‚ Ù…Ù‚Ø§Ù„Ù‡
            })


    def get_cost_for_sample(self, index: int):
        """
        Ú¯Ø±ÙØªÙ† Ù‡Ø²ÛŒÙ†Ù‡ ØªØµÙ…ÛŒÙ…â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø±Ú©ÙˆØ±Ø¯ iØ§Ù….
        Ø®Ø±ÙˆØ¬ÛŒ ÛŒÚ© Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø§ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ PP, PN, NP, NN (Ùˆ Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ BP, BN)
        """
        return self.cost_matrix[index]

    def get_all_costs(self):
        """ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ Ú©Ù„ cost_matrix Ø¨Ù‡ ØµÙˆØ±Øª Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒâ€ŒÙ‡Ø§. """
        return self.cost_matrix


###########################################
# Ú¯Ø§Ù… Ú†Ù‡Ø§Ø±Ù…: Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú†Ù†Ø¯Ù‡Ø¯ÙÙ‡ Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ NSGA-II (pymoo)
###########################################
import numpy as np
from pymoo.algorithms.moo.nsga2 import NSGA2
from pymoo.core.problem import Problem
from pymoo.optimize import minimize


class ParsianThresholdNSGA2:
    """
    ÛŒÚ© Ú©Ù„Ø§Ø³ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú¯Ø§Ù… Ú†Ù‡Ø§Ø±Ù… ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø³Ù‡â€ŒØ·Ø±ÙÙ‡ Ø¨Ù‡ ØµÙˆØ±Øª Ú†Ù†Ø¯Ù‡Ø¯ÙÙ‡:
      - Ù‡Ø¯Ù Ø§ÙˆÙ„: Ú©Ù…ÛŒÙ†Ù‡â€ŒÚ©Ø±Ø¯Ù† Ù‡Ø²ÛŒÙ†Ù‡ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ
      - Ù‡Ø¯Ù Ø¯ÙˆÙ…: Ú©Ù…ÛŒÙ†Ù‡â€ŒÚ©Ø±Ø¯Ù† Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù†Ø§Ø­ÛŒÙ‡ Ù…Ø±Ø²ÛŒ (BND)
      Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… NSGA-II Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ pymoo.

    ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§:
      - probabilities_test: Ø¢Ø±Ø§ÛŒÙ‡ Ø§Ø­ØªÙ…Ø§Ù„ Ù†Ú©ÙˆÙ„ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª
      - cost_matrix: Ø¢Ø±Ø§ÛŒÙ‡ ÛŒØ§ Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù‡Ø²ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø±Ú©ÙˆØ±Ø¯
           Ù…Ø«Ø§Ù„: cost_matrix[i] = {"PP": cost_if_true_and_decide_positive,
                                   "PN": cost_if_false_and_decide_positive,
                                   "NP": cost_if_true_and_decide_negative,
                                   "NN": cost_if_false_and_decide_negative,
                                   ...}
      - true_labels: Ø¢Ø±Ø§ÛŒÙ‡ Ø¨Ø±Ú†Ø³Ø¨ ÙˆØ§Ù‚Ø¹ÛŒ (Û° ÛŒØ§ Û±)
      - pop_size: Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¬Ù…Ø¹ÛŒØª Ø¨Ø±Ø§ÛŒ NSGA2
      - n_gen: ØªØ¹Ø¯Ø§Ø¯ Ù†Ø³Ù„ (iteration) Ø¨Ø±Ø§ÛŒ NSGA2
    """

    def __init__(self, probabilities_test: np.ndarray, cost_matrix: list, true_labels: np.ndarray, pop_size=50,
                 n_gen=100, step_bnd=False):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
         - step_bnd: Ø§Ú¯Ø± True Ø¨Ø§Ø´Ø¯ØŒ objective Ø¯ÙˆÙ… Ø±Ø§ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ BND Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
                     Ø§Ú¯Ø± False Ø¨Ø§Ø´Ø¯ØŒ Ù†Ø³Ø¨Øª Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ BND Ø¨Ù‡ Ú©Ù„ Ø±Ø§ Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
                     (Ù‡Ø± Ø¯Ùˆ Ø±ÙˆÛŒÚ©Ø±Ø¯ Ù…Ù…Ú©Ù† Ø§Ø³Øª.)
        """
        self.probabilities_test = probabilities_test  # Ø§Ø­ØªÙ…Ø§Ù„ Ù†Ú©ÙˆÙ„
        self.cost_matrix = cost_matrix  # Ø²ÛŒØ§Ù† Ù‡Ø± Ø±Ú©ÙˆØ±Ø¯
        self.true_labels = true_labels
        self.pop_size = pop_size
        self.n_gen = n_gen
        self.step_bnd = step_bnd

        self.best_solutions = None  # Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§Ø±ØªÙˆ Ø¨Ù‡â€ŒØ¯Ø³Øªâ€ŒØ¢Ù…Ø¯Ù‡
        self.front_costs = None  # Ù…Ù‚Ø¯Ø§Ø± Ø§Ù‡Ø¯Ø§Ù Ø¯Ø± Ù¾Ø§Ø±ØªÙˆ
        self.problem_instance = None

    def _decision_cost_for_sample(self, i: int, u: float, v: float) -> float:
        """
        ÙØ±Ù…ÙˆÙ„ Ú©Ø§Ù…Ù„Ù Ù…Ù‚Ø§Ù„Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡Ù” Ù‡Ø²ÛŒÙ†Ù‡Ù” ØªØµÙ…ÛŒÙ… ÛŒÚ© Ø±Ú©ÙˆØ±Ø¯.
        âŠ Ø§Ø¨ØªØ¯Ø§ Î±_i Ùˆ Î²_i Ø±Ø§ Ø¨Ø§ u , v Ùˆ Ø¶Ø±Ø§ÛŒØ¨ Î» Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
        â‹ Ø³Ù¾Ø³ ØªØµÙ…ÛŒÙ… (POSÂ /Â NEGÂ /Â BND) Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù‡ Ùˆ Ù‡Ø²ÛŒÙ†Ù‡ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†ÛŒÙ….
        """
        p_i = self.probabilities_test[i]
        y_i = self.true_labels[i]  # 0 ÛŒØ§ 1
        lam = self.cost_matrix[i]  # {'PP','PN','NP','NN'}

        # ---- Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€‘Ø§ÛŒ (Î±_i , Î²_i) Ù‡Ù…Ø§Ù† Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ù…Ù‚Ø§Ù„Ù‡ ----
        alpha_i = (lam["PN"] - v * lam["PN"]) / ((lam["PN"] - v * lam["PN"]) + (u * lam["NP"]))
        beta_i = (v * lam["PN"]) / ((v * lam["PN"]) + (lam["NP"] - u * lam["NP"]))

        # ---- ØªØµÙ…ÛŒÙ… Ø³Ù‡â€‘Ú¯Ø§Ù†Ù‡ ----
        if p_i >= alpha_i:  # POS
            return lam["PP"] if y_i == 1 else lam["PN"]
        elif p_i <= beta_i:  # NEG
            return lam["NP"] if y_i == 1 else lam["NN"]
        else:  # BND  (Ù‡Ø²ÛŒÙ†Ù‡Ù” Ù…Ø±Ø²ÛŒ Ø·Ø¨Ù‚ Ù…Ù‚Ø§Ù„Ù‡ Î»_BP , Î»_BN)
            bp_cost = 0.25 * lam["NP"]  # Î»_BP = 0.25Â Î»_NP
            bn_cost = 0.25 * lam["PN"]  # Î»_BN = 0.25Â Î»_PN
            return bp_cost if y_i == 1 else bn_cost

    def _boundary_count_for_solution(self, alpha, beta):
        """
        ØªØ¹Ø¯Ø§Ø¯ (ÛŒØ§ Ù†Ø³Ø¨Øª) Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ Ù…Ø±Ø²ÛŒ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù†Ø¯:
         ÛŒØ¹Ù†ÛŒ sample i Ú©Ù‡ p_i âˆˆ (beta, alpha)
        """
        p = self.probabilities_test
        bnd_mask = (p > beta) & (p < alpha)
        bnd_count = np.sum(bnd_mask)
        if self.step_bnd:
            return bnd_count  # ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø²ÛŒ
        else:
            return bnd_count / len(p)  # Ù†Ø³Ø¨Øª Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø²ÛŒ Ø¨Ù‡ Ú©Ù„

    class ThresholdOptimizationProblem(Problem):
        """
        Ú©Ù„Ø§Ø³ Ù…Ø³Ø¦Ù„Ù‡ pymoo Ø¨Ø±Ø§ÛŒ NSGA-II.
        n_var=2 => (alpha, beta)
        n_obj=2 => Ù‡Ø¯Ù Ø§ÙˆÙ„: Ù‡Ø²ÛŒÙ†Ù‡ØŒ Ù‡Ø¯Ù Ø¯ÙˆÙ…: Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù†Ø§Ø­ÛŒÙ‡ Ù…Ø±Ø²ÛŒ
        n_constr=1 => alpha >= beta  => beta - alpha <= 0
        xl=[0,0], xu=[1,1] => alpha,beta âˆˆ [0,1]
        """

        def __init__(self, outer, ):
            """
            - outer: ÛŒÚ© Ø§Ø´Ø§Ø±Ù‡ Ø¨Ù‡ Ú©Ù„Ø§Ø³ Ø¨ÛŒØ±ÙˆÙ†ÛŒ ParsianThresholdNSGA2
                     ØªØ§ Ø¨ØªÙˆØ§Ù†ÛŒÙ… Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ probabilities_test Ùˆ ... Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ….
            """
            super().__init__(n_var=2, n_obj=2, n_constr=1, xl=np.array([0.0, 0.0]), xu=np.array([1.0, 1.0]),
                             type_var=np.double)
            self.outer = outer  # Ø§Ø±Ø¬Ø§Ø¹ Ø¨Ù‡ Ú©Ù„Ø§Ø³ Ø¨ÛŒØ±ÙˆÙ†ÛŒ

        def _evaluate(self, X, out, *args, **kwargs):
            """
            Ø§Ú©Ù†ÙˆÙ† Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø±Ø§Ù‡â€ŒØ­Ù„ (u,v) Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Î±áµ¢,Î²áµ¢ Ø±Ø§ Â«Ø¨Ø±Ø§ÛŒ ØªÚ©â€ŒØªÚ© Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§Â» Ù…ÛŒâ€ŒØ³Ø§Ø²ÛŒÙ….
            Î»_BP = uÂ·Î»_NP , Î»_BN = vÂ·Î»_PN  (ÙØ±Ù…ÙˆÙ„ 4 Ù…Ù‚Ø§Ù„Ù‡)
            Î±áµ¢ , Î²áµ¢ Ø·Ø¨Ù‚ ÙØ±Ù…ÙˆÙ„â€ŒÙ‡Ø§ÛŒ (2) Ùˆ (3) Ù…Ù‚Ø§Ù„Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
            objective1 = Ù…Ø¬Ù…ÙˆØ¹ Ø²ÛŒØ§Ù†Ù Ø³Ù‡â€Œâ€‘Ø±Ø§Ù‡Ù‡
            objective2 = Î£(Î±áµ¢ âˆ’ Î²áµ¢)  (Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ¶Ø§ÛŒ Ù…Ø±Ø²ÛŒ)
            constraint : u+v â‰¤ 1
            """
            n_sol = X.shape[0]
            f1 = np.zeros(n_sol)  # total cost
            f2 = np.zeros(n_sol)  # boundary size
            g = np.zeros((n_sol, 1))  # u+v -1 â‰¤ 0

            p = self.outer.probabilities_test
            y_true = self.outer.true_labels
            costs = self.outer.cost_matrix

            for k in range(n_sol):
                u, v = X[k]
                tot_cost, bnd_size = 0.0, 0.0

                for i in range(len(p)):
                    lam = costs[i]
                    lam_BP = u * lam["NP"]
                    lam_BN = v * lam["PN"]

                    alpha = (lam["PN"] - lam_BN) / ((lam["PN"] - lam_BN) + (lam_BP - lam["PP"]))
                    beta = (lam_BN - lam["NN"]) / ((lam_BN - lam["NN"]) + (lam["NP"] - lam_BP))

                    # decision + cost
                    if p[i] >= alpha:  # POS
                        tot_cost += lam["PP"] if y_true[i] == 1 else lam["PN"]
                    elif p[i] <= beta:  # NEG
                        tot_cost += lam["NP"] if y_true[i] == 1 else lam["NN"]
                    else:  # BND
                        tot_cost += lam_BP if y_true[i] == 1 else lam_BN
                        bnd_size += (alpha - beta)

                f1[k] = tot_cost
                f2[k] = bnd_size
                g[k, 0] = u + v - 1.0  # Ù‡Ù…Ø§Ù† Ù‚ÛŒÙˆØ¯ (6) Ù…Ù‚Ø§Ù„Ù‡

            out["F"] = np.column_stack([f1, f2])
            out["G"] = g


    def optimize(self):
        """
        Ø§Ø¬Ø±Ø§ÛŒ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… NSGA-II Ø¨Ø±Ø§ÛŒ Ú©Ù…ÛŒÙ†Ù‡â€ŒÚ©Ø±Ø¯Ù† [cost, boundary_size]
        Ø¨Ø§ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª alpha >= beta.
        """
        logging.info("ğŸ”µ Ø´Ø±ÙˆØ¹ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú†Ù†Ø¯Ù‡Ø¯ÙÙ‡ Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ NSGA-II...")

        # Ø³Ø§Ø®Øª Ù†Ù…ÙˆÙ†Ù‡ Ù…Ø³Ø¦Ù„Ù‡
        self.problem_instance = self.ThresholdOptimizationProblem(self)

        # Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… NSGA2
        algo = NSGA2(pop_size=self.pop_size)

        # Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
        res = minimize(self.problem_instance, algo, ("n_gen", self.n_gen), seed=42, verbose=False)

        self.front_costs = res.F  # Ù‡Ø¯Ùâ€ŒÙ‡Ø§ÛŒ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§Ø±ØªÙˆ
        self.best_solutions = res.X  # Ø®ÙˆØ¯ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§ (alpha,beta) Ø¯Ø± Ù¾Ø§Ø±ØªÙˆ
        logging.info("âœ… NSGA-II Ø¨Ù‡ Ø§ØªÙ…Ø§Ù… Ø±Ø³ÛŒØ¯. ØªØ¹Ø¯Ø§Ø¯ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§Ø±ØªÙˆ: {}".format(len(self.front_costs)))

    def get_pareto_front(self):
        """
        Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ (solutions, objectives) = (self.best_solutions, self.front_costs)
        Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ù‡Ø± solutions[i] = [alpha_i, beta_i]
              each objectives[i] = [cost_i, boundary_i]
        """
        return self.best_solutions, self.front_costs

    def get_final_solution(self):
        """
        Ø±Ø§Ù‡â€ŒØ­Ù„ Ù†Ù‡Ø§ÛŒÛŒ Ù‡Ù…Ø§Ù† Ø¹Ø¶ÙˆÛŒ Ø§Ø² Ø¬Ø¨Ù‡Ù‡Ù” Ù¾Ø§Ø±ØªÙˆ Ø§Ø³Øª Ú©Ù‡:
           â€¢  Ù†Ø³Ø¨Øª BND â‰¤ 5Ùª Ú©Ù„ Ø¯Ø§Ø¯Ù‡
           â€¢  Ø¨ÛŒØ´ØªØ±ÛŒÙ† Balancedâ€‘Accuracy Ø±Ø§ Ø¯Ø§Ø±Ø¯
           â€¢  (Ø¯Ø± ØµÙˆØ±Øª ØªØ³Ø§ÙˆÛŒ) Ú©Ù…ÛŒÙ†Ù‡Ù” Totalâ€‘Cost Ø±Ø§ Ø¯Ø§Ø±Ø¯
        """
        if self.best_solutions is None:
            raise RuntimeError("Ø§Ø¨ØªØ¯Ø§ optimize Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.")

        best_idx, best_balacc, best_cost = None, -1.0, np.inf

        for idx, (u, v) in enumerate(self.best_solutions):
            preds = []
            for i in range(len(self.probabilities_test)):
                p_i = self.probabilities_test[i]
                lam = self.cost_matrix[i]
                alpha = (lam["PN"] - v * lam["PN"]) / ((lam["PN"] - v * lam["PN"]) + (u * lam["NP"]))
                beta = (v * lam["PN"]) / ((v * lam["PN"]) + (lam["NP"] - u * lam["NP"]))
                preds.append(1 if p_i >= alpha else (0 if p_i <= beta else -1))

            preds_arr = np.array(preds)
            bnd_ratio = np.mean(preds_arr == -1)
            if bnd_ratio > 0.05:  # Ø´Ø±Ø· Ù…Ù‚Ø§Ù„Ù‡
                continue

            preds_arr[preds_arr == -1] = 0  # Ø¨Ø±Ø¢ÙˆØ±Ø¯ Ø³Ø±ÛŒØ¹ Ø¨Ø±Ø§ÛŒ BalAcc
            cm = confusion_matrix(self.true_labels, preds_arr)
            TN, FP, FN, TP = cm.ravel()
            balacc = 0.5 * ((TP / (TP + FN + 1e-9)) + (TN / (TN + FP + 1e-9)))

            if (balacc > best_balacc) or (np.isclose(balacc, best_balacc) and self.front_costs[idx, 0] < best_cost):
                best_idx, best_balacc, best_cost = idx, balacc, self.front_costs[idx, 0]

        return self.best_solutions[best_idx], self.front_costs[best_idx]


###########################################
# Ú¯Ø§Ù… Ù¾Ù†Ø¬Ù…: ØªÙ‚Ø³ÛŒÙ… Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ POS/NEG/BND (Three-Way Decision)
###########################################

class ParsianThreeWayDecision:
    def __init__(self, probabilities_test: np.ndarray, cost_matrix: list,
                 alpha_beta_pair: Tuple[float,float]):
        self.prob = probabilities_test
        self.cost = cost_matrix
        self.u, self.v = alpha_beta_pair   # Ù‡Ù…Ø§Ù† (u*,v*)
        self.decisions = None

    def _alpha_beta_i(self, lam):
        lam_BP = self.u * lam["NP"]
        lam_BN = self.v * lam["PN"]
        Î± = (lam["PN"] - lam_BN) / ((lam["PN"]-lam_BN) + (lam_BP - lam["PP"]))
        Î² = (lam_BN - lam["NN"]) / ((lam_BN - lam["NN"]) + (lam["NP"] - lam_BP))
        return Î±, Î²

    def apply_three_way_decision(self):
        dec = np.zeros(len(self.prob), dtype=int)
        for i, p_i in enumerate(self.prob):
            Î±, Î² = self._alpha_beta_i(self.cost[i])
            if p_i >= Î±:      dec[i] = 1
            elif p_i <= Î²:    dec[i] = 0
            else:             dec[i] = -1
        self.decisions = dec
        return dec

    # ---------- Ø´Ù…Ø§Ø±Ø´ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ --------------
    def get_decision_counts(self):
        """
        Ø¨Ø±Ú¯Ø´Øª Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ {1:POS , 0:NEG , -1:BND}
        """
        if self.decisions is None:
            self.apply_three_way_decision()
        uniq, cnt = np.unique(self.decisions, return_counts=True)
        return dict(zip(uniq, cnt))

###########################################
# Ú¯Ø§Ù… Ø´Ø´Ù…: ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ BND
#          (Ù…Ø«Ù„Ø§Ù‹ Ø¨Ø§ Ø§Ø³ØªÚ©ÛŒÙ†Ú¯ ÛŒØ§ Ù…Ø¯Ù„ Ú©Ù…Ú©ÛŒ Ø¯ÛŒÚ¯Ø±)
###########################################
from sklearn.ensemble import StackingClassifier, RandomForestClassifier, BaggingClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier


class ParsianBNDResolver:
    """
    Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø±Ø§ Ú©Ù‡ Ø¯Ø± Ú¯Ø§Ù… Ù¾Ù†Ø¬Ù… Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ BND ÙˆØ§Ù‚Ø¹ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ØŒ
    Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ùˆ Ø¨Ø§ ÛŒÚ© Ù…Ø¯Ù„ Ø§Ø¶Ø§ÙÛŒØŒ ØªØµÙ…ÛŒÙ… Ù‚Ø·Ø¹ÛŒ (POS ÛŒØ§ NEG) Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.

    Ø¯Ø± Ø§ÛŒÙ† Ù¾Ú˜ÙˆÙ‡Ø´ØŒ Ø¨Ù‡ Ù…Ù†Ø¸ÙˆØ± ØªØ¹ÛŒÛŒÙ† ØªØµÙ…ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø²ÛŒØŒ
    Ø§Ø² Ø¯Ùˆ Ø±ÙˆÛŒÚ©Ø±Ø¯ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¬Ù…Ø¹ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Stacking Ùˆ Bagging Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª.
    Ø§ÛŒÙ† Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø± Ø§Ù…Ú©Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ ØªØ§ Ø¯Ø± Ø´Ø±Ø§ÛŒØ· Ù…Ø®ØªÙ„Ù Ø¯Ø§Ø¯Ù‡ØŒ
    Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´ ØªÚ©Ù…ÛŒÙ„ÛŒ Ø¬Ù‡Øª Ú©Ø§Ù‡Ø´ Ø¹Ø¯Ù… Ù‚Ø·Ø¹ÛŒØª Ø¯Ø± ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø±Ø§ Ø¨Ù‡ Ú©Ø§Ø± Ú¯ÛŒØ±Ø¯.
    """

    def __init__(self, x_train_all: pd.DataFrame, y_train_all: pd.Series, model_type="bagging"):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
          - x_train_all, y_train_all: Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø§ØµÙ„ÛŒ ÛŒØ§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø²ÛŒ
          - model_type: Ù†ÙˆØ¹ Ù…Ø¯Ù„ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø²ÛŒ Ø¨Ù‡â€ŒÚ©Ø§Ø± Ø¨Ø±ÛŒÙ…
                       (Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„ "stacking" ÛŒØ§ "bagging")
        """
        self.x_train_all = x_train_all
        self.y_train_all = y_train_all
        self.model_type = model_type
        self.classifier = None

    def fit_bnd_model(self):
        """
        Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ØŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡ (Stacking ÛŒØ§ Bagging)ØŒ Ù…Ø¯Ù„ ØªÚ©Ù…ÛŒÙ„ÛŒ Ø¬Ù‡Øª
        ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø²ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

        - Ø¯Ø± ØµÙˆØ±Øª Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ StackingØŒ Ú†Ù†Ø¯ÛŒÙ† Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡ (Ù…Ø§Ù†Ù†Ø¯ Random Forest Ùˆ XGBoost)
          Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ ÛŒÚ© Ù…Ø¯Ù„ Ù…ØªØ§ (Ù…Ø§Ù†Ù†Ø¯ Logistic Regression) Ø¨Ù‡â€ŒØµÙˆØ±Øª ÛŒÚ© Ú†Ø§Ø±Ú†ÙˆØ¨ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
          Ú†Ù†Ø¯Ù„Ø§ÛŒÙ‡ Ø¢Ù…ÙˆØ²Ø´ Ù…ÛŒâ€ŒØ¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.
        - Ø¯Ø± ØµÙˆØ±Øª Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ BaggingØŒ Ø§Ø² ÛŒÚ© Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡ Ù…Ø§Ù†Ù†Ø¯ Ø¯Ø±Ø®Øª ØªØµÙ…ÛŒÙ… Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù†
          Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¬Ù…Ø¹ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ùˆ Ø¨Ø§ Ø¨Ù‡Ø±Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø±ÙˆÛŒÚ©Ø±Ø¯ BaggingØŒ ØªÙˆØ§Ù† Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
          Ø¯Ø± Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø²ÛŒ Ø§Ø±ØªÙ‚Ø§ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯.

        Ø§ÛŒÙ† Ø±ÙˆÛŒÚ©Ø±Ø¯ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¬Ù…Ø¹ÛŒ Ø¨Ø§ Ù‡Ø¯Ù Ø¨Ù‡Ø¨ÙˆØ¯ Ù‚Ø§Ø¨Ù„ÛŒØª ØªØ¹Ù…ÛŒÙ… Ù…Ø¯Ù„ Ùˆ Ú©Ø§Ù‡Ø´ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ù†Ø§Ø´ÛŒ Ø§Ø²
        ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø¯Ø± Ù†ÙˆØ§Ø­ÛŒ Ù†Ø§Ù…Ø·Ù…Ø¦Ù† (Boundary) Ø¨Ù‡â€ŒÚ©Ø§Ø± Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡ Ø§Ø³Øª.
        """
        if self.model_type.lower() == "stacking":
            # ØªÙ†Ø¸ÛŒÙ… Ú†Ø§Ø±Ú†ÙˆØ¨ Ù…Ø¯Ù„ Stacking: Ú†Ù†Ø¯ÛŒÙ† Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡ Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ ÛŒÚ© Ù…ØªØ§ Ù…Ø¯Ù„
            base_estimators = [("rf", RandomForestClassifier(n_estimators=50, random_state=42)),
                               ("xgb", XGBClassifier(eval_metric="logloss", random_state=42))]
            meta_estimator = LogisticRegression(max_iter=1000, random_state=42)
            self.classifier = StackingClassifier(estimators=base_estimators, final_estimator=meta_estimator, cv=5,
                                                 n_jobs=-1)
        elif self.model_type.lower() == "bagging":
            base_estimator = DecisionTreeClassifier(
                criterion="gini",
                max_depth=None,  # Ø§Ø¬Ø§Ø²Ù‡Ù” Ø±Ø´Ø¯ Ú©Ø§Ù…Ù„
                min_samples_leaf=2,  # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² overâ€‘fitting Ø±ÛŒØ²
                class_weight="balanced",  # Ø¬Ø¨Ø±Ø§Ù† Ú©Ù„Ø§Ø³ Ø§Ù‚Ù„ÛŒØª
                random_state=42
            )

            # Û²) BaggingClassifier Ø¨Ø§ Û²Û°Û° Ø¯Ø±Ø®ØªØŒ Ø¨ÙˆØªâ€ŒØ§Ø³ØªØ±Ù¾ Ù‡Ù… Ø±ÙˆÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ùˆ Ù‡Ù… Ø±ÙˆÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
            self.classifier = BaggingClassifier(
                estimator=base_estimator,
                n_estimators=200,  # ØªØ¹Ø¯Ø§Ø¯ Ú©ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ§Ø¯ØªØ±
                max_samples=0.8,  # Û¸Û°Ùª Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ù‡Ø± Ú©ÛŒÙ
                max_features=0.8,  # Û¸Û°Ùª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ù‡Ø± Ú©ÛŒÙ
                bootstrap=True,
                bootstrap_features=True,  # Ø¨ÙˆØªâ€ŒØ§Ø³ØªØ±Ù¾ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†ÙˆØ¹â€ŒØ³Ø§Ø²ÛŒ Ø¨ÛŒØ´ØªØ±
                oob_score=True,  # Ø¨Ø±Ø¢ÙˆØ±Ø¯ Ø®Ø·Ø§ÛŒ Ø®Ø§Ø±Ø¬â€ŒØ§Ø²-Ú©ÛŒÙ
                n_jobs=-1,
                random_state=42,
                verbose=0
            )
        else:
            raise ValueError("ÙØ¹Ù„Ø§Ù‹ ÙÙ‚Ø· Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ 'stacking' Ùˆ 'bagging' Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.")

        logging.info(f"ğŸ”µ Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ BNDResolver ({self.model_type.capitalize()})...")
        self.classifier.fit(self.x_train_all, self.y_train_all)
        logging.info("âœ… Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ BNDResolver Ú©Ø§Ù…Ù„ Ø´Ø¯.")

    def resolve_bnd_samples(self, x_test: pd.DataFrame, decisions_final: np.ndarray):
        """
        Ø§ÛŒÙ† Ù…ØªØ¯ Ø¨Ù‡â€ŒÙ…Ù†Ø¸ÙˆØ± Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ØªØµÙ…ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ Ù…Ø±Ø²ÛŒ (BND)
        Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ‡â€ŒØ§Ù†Ø¯ØŒ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø§Ø² Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ Ø¬Ù‡Øª ØªØ¹ÛŒÛŒÙ† Ø·Ø¨Ù‚Ù‡ (POS ÛŒØ§ NEG) Ø¨Ø±Ø§ÛŒ
        Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø²ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

        ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§:
          - x_test: Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…ÙˆÙ† Ú©Ø§Ù…Ù„
          - decisions_final: Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ ØªØµÙ…ÛŒÙ… (POS=1, NEG=0, BND=-1)

        Ø®Ø±ÙˆØ¬ÛŒ:
          - decisions_updated: Ø¢Ø±Ø§ÛŒÙ‡ Ù†Ù‡Ø§ÛŒÛŒ ØªØµÙ…ÛŒÙ…â€ŒÙ‡Ø§ Ú©Ù‡ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø²ÛŒ Ù†ÛŒØ² Ø¨Ù‡ ÛŒÚ©ÛŒ Ø§Ø² Ø¯Ùˆ Ú©Ù„Ø§Ø³
            Ù‚Ø·Ø¹ÛŒ (POS ÛŒØ§ NEG) ØªØ®ØµÛŒØµ ÛŒØ§ÙØªÙ‡â€ŒØ§Ù†Ø¯.
        """
        bnd_indices = np.where(decisions_final == -1)[0]
        logging.info(f"ğŸ”µ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ BND: {len(bnd_indices)}")

        if len(bnd_indices) == 0:
            logging.info("Ù‡ÛŒÚ† Ù†Ù…ÙˆÙ†Ù‡ Ù…Ø±Ø²ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. ØªØºÛŒÛŒØ±ÛŒ Ø§Ø¹Ù…Ø§Ù„ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
            return decisions_final

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù…Ù†Ø·Ù‚Ù‡ Ù…Ø±Ø²ÛŒ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…ÙˆÙ†
        x_test_bnd = x_test.iloc[bnd_indices]
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø·Ø¨Ù‚Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ ØªÚ©Ù…ÛŒÙ„ÛŒ
        y_pred_bnd = self.classifier.predict(x_test_bnd)

        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ ØªØµÙ…ÛŒÙ… Ù†Ù‡Ø§ÛŒÛŒ
        decisions_updated = decisions_final.copy()
        for idx, pred in zip(bnd_indices, y_pred_bnd):
            decisions_updated[idx] = pred  # ØªØ¹ÛŒÛŒÙ† Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ù‡ Ø§Ø²Ø§ÛŒ Ù‡Ø± Ù†Ù…ÙˆÙ†Ù‡ (0 ÛŒØ§ 1)
        return decisions_updated


###########################################
# Ú¯Ø§Ù… Ù‡ÙØªÙ…: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ú¯Ø²Ø§Ø±Ø´ (Final Evaluation)
###########################################
import numpy as np
from sklearn.metrics import roc_auc_score


class ParsianFinalEvaluator:
    """
    Ø¯Ø± Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ØŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ ØªØµÙ…ÛŒÙ… (Ø¨Ø¹Ø¯ Ø§Ø² Ú¯Ø§Ù… Ø´Ø´Ù…) Ø±Ø§ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ ÙˆØ§Ù‚Ø¹ÛŒ
    Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ùˆ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ Ú¯Ø²Ø§Ø±Ø´ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ….
    """

    def __init__(self, true_labels: np.ndarray, final_decisions: np.ndarray, probabilities_test: np.ndarray = None,
                 cost_matrix: list = None):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
          - true_labels: Ø¢Ø±Ø§ÛŒÙ‡ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ ØªØ³Øª (Û° ÛŒØ§ Û±)
          - final_decisions: Ø¢Ø±Ø§ÛŒÙ‡ ØªØµÙ…ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ (Û°=NEG, Û±=POS)
          - probabilities_test: Ø§Ú¯Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… AUC ÛŒØ§ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„Ø§ØªÛŒ Ø±Ø§ Ø­Ø³Ø§Ø¨ Ú©Ù†ÛŒÙ…
          - cost_matrix: Ø§Ú¯Ø± Ø¨Ø®ÙˆØ§Ù‡ÛŒÙ… Ù‡Ø²ÛŒÙ†Ù‡ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø±Ø§ Ù†ÛŒØ² Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ù…Ø§ÛŒÛŒÙ…. (Ù…Ø«Ù„Ø§Ù‹ Ø®Ø±ÙˆØ¬ÛŒ Ú¯Ø§Ù… Ø³ÙˆÙ…)
        """
        self.true_labels = true_labels
        self.final_decisions = final_decisions
        self.probabilities_test = probabilities_test
        self.cost_matrix = cost_matrix

    def evaluate_metrics(self):
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ù†Ø¸ÛŒØ±:
         - Balanced Accuracy
         - Precision, Recall, F1
         - AUC (Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ probabilities_test)
         - Ù‡Ø²ÛŒÙ†Ù‡ ØªØµÙ…ÛŒÙ… (Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ cost_matrix)
        Ø®Ø±ÙˆØ¬ÛŒ: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø´Ø§Ù…Ù„ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§
        """

        if len(self.true_labels) != len(self.final_decisions):
            raise ValueError("Ø·ÙˆÙ„ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§ ØªØµÙ…ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ù‡Ù…Ø®ÙˆØ§Ù†ÛŒ Ù†Ø¯Ø§Ø±Ø¯.")

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ confusion matrix
        cm = confusion_matrix(self.true_labels, self.final_decisions)
        # cm Ø³Ø§Ø®ØªØ§Ø± [[TN, FP], [FN, TP]]
        TN, FP, FN, TP = cm.ravel()

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Balanced Accuracy
        # = 0.5 * (TP/(TP+FN) + TN/(TN+FP))
        sensitivity = TP / (TP + FN) if (TP + FN) != 0 else 0
        specificity = TN / (TN + FP) if (TN + FP) != 0 else 0
        b_acc = 0.5 * (sensitivity + specificity)

        # Precision, Recall, F1
        precision = precision_score(self.true_labels, self.final_decisions, zero_division=0)
        recall = recall_score(self.true_labels, self.final_decisions, zero_division=0)
        f1 = f1_score(self.true_labels, self.final_decisions, zero_division=0)

        # AUC
        auc_val = None
        if self.probabilities_test is not None:
            # Ø§Ú¯Ø± probabilities_test Ø®Ø±ÙˆØ¬ÛŒ predict_proba (Ø³ØªÙˆÙ† Ú©Ù„Ø§Ø³ Û±) Ø¨Ø§Ø´Ø¯:
            try:
                auc_val = roc_auc_score(self.true_labels, self.probabilities_test)
            except Exception:
                auc_val = None

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø²ÛŒÙ†Ù‡ ØªØµÙ…ÛŒÙ…ØŒ Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ cost_matrix Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ…
        # ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ cost_matrix[i] = {"PP", "PN", "NP", "NN"}
        # Ùˆ final_decisions[i] = 1 => POS, 0 => NEG
        # true_labels[i] = 1 => Ù†Ù…ÙˆÙ†Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ù†Ú©ÙˆÙ„, 0 => ØºÛŒØ±Ù†Ú©ÙˆÙ„
        total_cost = None
        if self.cost_matrix is not None:
            if len(self.cost_matrix) != len(self.true_labels):
                logging.warning("Ø·ÙˆÙ„ cost_matrix Ø¨Ø§ Ø¯Ø§Ø¯Ù‡Ù” ØªØ³Øª Ù‡Ù…Ø®ÙˆØ§Ù†ÛŒ Ù†Ø¯Ø§Ø±Ø¯Ø› Ù‡Ø²ÛŒÙ†Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
            else:
                total_cost_calc = 0.0
                for i in range(len(self.true_labels)):
                    y_true = self.true_labels[i]
                    y_pred = self.final_decisions[i]
                    costs = self.cost_matrix[i]
                    # Ø§Ú¯Ø± y_pred=1 Ùˆ y_true=1 => PP
                    # Ø§Ú¯Ø± y_pred=1 Ùˆ y_true=0 => PN
                    # Ø§Ú¯Ø± y_pred=0 Ùˆ y_true=1 => NP
                    # Ø§Ú¯Ø± y_pred=0 Ùˆ y_true=0 => NN
                    if y_pred == 1 and y_true == 1:
                        total_cost_calc += costs["PP"]
                    elif y_pred == 1 and y_true == 0:
                        total_cost_calc += costs["PN"]
                    elif y_pred == 0 and y_true == 1:
                        total_cost_calc += costs["NP"]
                    elif y_pred == 0 and y_true == 0:
                        total_cost_calc += costs["NN"]
                total_cost = total_cost_calc

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ GM Ø¨Ù‡ ØµÙˆØ±Øª sqrt((TP/(TP+FN)) * (TN/(TN+FP)))
        gm = np.sqrt((TP / (TP + FN)) * (TN / (TN + FP))) if (TP + FN) != 0 and (TN + FP) != 0 else 0

        metrics_dict = {"ModelName": "Proposed-3WD", "TN": TN, "FP": FP, "FN": FN, "TP": TP, "BalancedAccuracy": b_acc,
                        "Precision": precision, "Recall": recall, "F1": f1, "GM": gm, "AUC": auc_val,
                        "TotalCost": total_cost}
        return metrics_dict


from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score
from math import sqrt
import logging


class ParsianMethodComparison:
    """
    Ø¯Ø± Ø§ÛŒÙ† Ú¯Ø§Ù…ØŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø±Ù‚ÛŒØ¨ (Baseline) Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
    Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ùˆ Ø³Ù¾Ø³ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø¨Ø§ Ø±ÙˆØ´ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ
    (Ø³Ù‡â€ŒØ·Ø±ÙÙ‡) Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø´ÙˆÙ†Ø¯.

    Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ:
      - TP, TN, FP, FN Ø§Ø² Ù…Ø§ØªØ±ÛŒØ³ Ø³Ø±Ø¯Ø±Ú¯Ù…ÛŒ
      - BalancedAccuracy = 0.5 * (TP/(TP+FN) + TN/(TN+FP))
      - Precision, Recall, F1 (ÛŒØ§ FM Ø¨Ø§ Î²=1)
      - GM = sqrt( (TP/(TP+FN)) * (TN/(TN+FP)) )
      - AUC (Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ Ø§Ø­ØªÙ…Ø§Ù„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ)
      - TotalCost (Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ cost_matrix)
    """

    def __init__(self, x_train: pd.DataFrame, y_train: pd.Series, x_test: pd.DataFrame, y_test: pd.Series,
                 cost_matrix: list = None, model_comparisons: dict = None):
        """
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
         - x_train, y_train: Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
         - x_test, y_test: Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª
         - cost_matrix: Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø²ÛŒÙ†Ù‡ ØªØµÙ…ÛŒÙ… (Ù…Ø§Ù†Ù†Ø¯ Ú¯Ø§Ù… Û³)
         - model_comparisons: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ { ModelName: model_object } Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø±Ù‚ÛŒØ¨
        """
        self.x_train = x_train
        self.y_train = y_train
        self.x_test = x_test
        self.y_test = y_test
        self.cost_matrix = cost_matrix

        if model_comparisons is None:
            from sklearn.naive_bayes import GaussianNB
            from sklearn.neighbors import KNeighborsClassifier
            from sklearn.linear_model import LogisticRegression
            from sklearn.neural_network import MLPClassifier
            from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier, \
                RandomForestClassifier, BaggingClassifier
            from lightgbm import LGBMClassifier
            from xgboost import XGBClassifier
            from sklearn.ensemble import StackingClassifier
            from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

            self.model_comparisons = {"Bayes": GaussianNB(), "KNN": KNeighborsClassifier(),
                                      "LR": LogisticRegression(max_iter=10_000), "NN": MLPClassifier(max_iter=300),
                                      "AdaBoost": AdaBoostClassifier(algorithm="SAMME"), "ERT": ExtraTreesClassifier(),
                                      "GBDT": GradientBoostingClassifier(), "LGBM": LGBMClassifier(verbose=-1),
                                      "RF": RandomForestClassifier(),
                                      "XGB": XGBClassifier(eval_metric='logloss', verbosity=0),
                                      "Stacking": StackingClassifier(estimators=[('lda', LinearDiscriminantAnalysis()),
                                                                                 ('knn', KNeighborsClassifier())],
                                                                     final_estimator=RandomForestClassifier()),
                                      "Bagging": BaggingClassifier(
                                          estimator=ExtraTreesClassifier(n_estimators=100, random_state=42),
                                          n_estimators=10, random_state=42)}
        else:
            self.model_comparisons = model_comparisons

        self.comparison_table = None

    def _compute_metrics(self, y_true, y_pred, y_prob=None, cost_matrix=None):
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯: TP, TN, FP, FN, BalancedAccuracy, Precision, Recall, F1, GM, AUC Ùˆ TotalCost.
        """
        cm = confusion_matrix(y_true, y_pred)
        TN, FP, FN, TP = cm.ravel()

        # Balanced Accuracy
        b_acc = 0.5 * ((TP / (TP + FN) if (TP + FN) > 0 else 0) + (TN / (TN + FP) if (TN + FP) > 0 else 0))

        # Precision Ùˆ Recall
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)

        # F1 (Ú©Ù‡ Ù‡Ù…Ø§Ù† FM Ø¨Ø§ Î²=1 Ø§Ø³Øª)
        f1 = f1_score(y_true, y_pred, zero_division=0)

        # GM
        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0
        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0
        gm = sqrt(sensitivity * specificity)

        # AUC
        auc_val = None
        if y_prob is not None:
            from sklearn.metrics import roc_auc_score
            try:
                auc_val = roc_auc_score(y_true, y_prob)
            except Exception:
                auc_val = None

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø²ÛŒÙ†Ù‡ ØªØµÙ…ÛŒÙ… (TotalCost)
        total_cost = None
        if cost_matrix is not None and len(cost_matrix) == len(y_true):
            tc = 0.0
            for i in range(len(y_true)):
                yi = y_true[i]
                yp = y_pred[i]
                costs = cost_matrix[i]
                if yi == 1 and yp == 1:
                    tc += costs["PP"]
                elif yi == 0 and yp == 1:
                    tc += costs["PN"]
                elif yi == 1 and yp == 0:
                    tc += costs["NP"]
                elif yi == 0 and yp == 0:
                    tc += costs["NN"]
            total_cost = tc

        return {"TP": TP, "TN": TN, "FP": FP, "FN": FN, "BalancedAccuracy": b_acc, "Precision": precision,
                "Recall": recall, "F1": f1, "GM": gm, "AUC": auc_val, "TotalCost": total_cost}

    def run_comparison(self):
        """
        Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„ Ø¯Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø±Ù‚ÛŒØ¨:
          - Ø¢Ù…ÙˆØ²Ø´ Ø±ÙˆÛŒ x_train, y_train
          - Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ x_test (y_pred Ùˆ Ø¯Ø± ØµÙˆØ±Øª Ø§Ù…Ú©Ø§Ù† y_prob)
          - Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§
          - Ø«Ø¨Øª Ù†ØªØ§ÛŒØ¬ Ø¯Ø± ÛŒÚ© DataFrame
        Ø®Ø±ÙˆØ¬ÛŒ: DataFrame Ø´Ø§Ù…Ù„ Ù†ØªØ§ÛŒØ¬ Ù…Ù‚Ø§ÛŒØ³Ù‡
        """
        logging.info("ğŸ”µ Ø´Ø±ÙˆØ¹ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ø³Ø§ÛŒØ± Ø±ÙˆØ´â€ŒÙ‡Ø§ (Ú¯Ø§Ù… Û¹) ...")
        results_list = []
        for model_name, model_obj in self.model_comparisons.items():
            model_obj.fit(self.x_train, self.y_train)

            y_pred = model_obj.predict(self.x_test)

            y_prob = None
            try:
                prob_mat = model_obj.predict_proba(self.x_test)
                y_prob = prob_mat[:, 1]
            except Exception:
                y_prob = None

            metrics = self._compute_metrics(y_true=self.y_test.values, y_pred=y_pred, y_prob=y_prob,
                                            cost_matrix=self.cost_matrix)
            metrics["ModelName"] = model_name
            results_list.append(metrics)

        df_results = pd.DataFrame(results_list)
        df_results.sort_values(by="BalancedAccuracy", ascending=False, inplace=True)
        self.comparison_table = df_results.reset_index(drop=True)
        logging.info("âœ… Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø±Ù‚ÛŒØ¨ Ù¾Ø§ÛŒØ§Ù† ÛŒØ§ÙØª.")
        return self.comparison_table

    def add_proposed_method_results(self, proposed_method_metrics: dict):
        """
        Ø§ÙØ²ÙˆØ¯Ù† Ù†ØªØ§ÛŒØ¬ Ø±ÙˆØ´ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨Ù‡ Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡.
        proposed_method_metrics Ø¨Ø§ÛŒØ¯ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø§ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ:
        ModelName, TP, TN, FP, FN, BalancedAccuracy, Precision, Recall, F1, GM, AUC, TotalCost
        Ø¨Ø§Ø´Ø¯.
        """
        self.comparison_table = pd.concat([self.comparison_table, pd.DataFrame([proposed_method_metrics])],
                                          ignore_index=True)
        self.comparison_table.sort_values(by="BalancedAccuracy", ascending=False, inplace=True)
        self.comparison_table.reset_index(drop=True, inplace=True)
        logging.info("ğŸ”µ Ù†ØªØ§ÛŒØ¬ Ø±ÙˆØ´ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ù‡Ù… Ø¨Ù‡ Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯.")

    def show_final_comparison(self):
        """
        Ù†Ù…Ø§ÛŒØ´ Ø¬Ø¯ÙˆÙ„ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ø± Ù„Ø§Ú¯ Ùˆ Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø¢Ù†.
        ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾Ø§Ù†Ø¯Ø§Ø² Ø¨Ù‡ Ú¯ÙˆÙ†Ù‡â€ŒØ§ÛŒ ØªØºÛŒÛŒØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ù‡Ù…Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø¨Ø¯ÙˆÙ† ellipsis Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯.
        """
        logging.info("ğŸ”¸ Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§:")
        logging.warning("\n" + str(self.comparison_table))
        return self.comparison_table

###########################################
# ØªØ³Øª Ú©Ù„ ÙØ±Ø¢ÛŒÙ†Ø¯ (Ø¯Ø± ØµÙˆØ±Øª Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø§ÛŒÙ† ÙØ§ÛŒÙ„)
###########################################
if __name__ == "__main__":
    os.environ["LOKY_MAX_CPU_COUNT"] = "8"
    visualizer = Plot()
    logging.basicConfig(level=logging.INFO)

    # Ø³Ø§Ø®Øª Ø¢Ø¨Ø¬Ú©Øª Ù…Ø®Ø²Ù† Ø¯Ø§Ø¯Ù‡ (LoanRepository)
    repo = LoanRepository()

    # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ (ParsianPreprocessingManager)
    prep_manager = ParsianPreprocessingManager(repository=repo, limit_records=49_000, label_column="LOAN_STATUS",
                                               imputation_strategy="mean",
                                               need_2_remove_highly_correlated_features=False,
                                               correlation_threshold=0.95, do_balance=True, test_size=0.2,
                                               random_state=42)

    x_train, y_train, x_test, y_test, original_df = prep_manager.step1_process_data()
    if x_train is None:
        logging.error("Ú¯Ø§Ù… Ø§ÙˆÙ„ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯.")
        exit(1)

    # visualizer.explained_variance(x_train)
    # visualizer.plot_pca_2d(x_train)
    # visualizer.plot_pca_3d(x_train)
    # visualizer.plot_tsne(x_train)
    # visualizer.draw_preprocessing_flowchart()

    # 2) Ø§Ø¬Ø±Ø§ÛŒ Ú¯Ø§Ù… Ø¯ÙˆÙ…: Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ùˆ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ Ù†Ú©ÙˆÙ„
    default_model = ParsianDefaultProbabilityModel(model_type="lightgbm", n_estimators=100, learning_rate=0.05,
                                                   random_state=42)
    default_model.fit_model(x_train, y_train)
    probabilities_test = default_model.predict_default_probability(x_test)

    visualizer.plot1(probabilities_test)

    logging.info(f"Ø§Ø­ØªÙ…Ø§Ù„ Ù†Ú©ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ† 5 Ù†Ù…ÙˆÙ†Ù‡: {probabilities_test[:5]}")
    logging.info("Ú¯Ø§Ù… Ø¯ÙˆÙ… (Ø¨Ø±Ø¢ÙˆØ±Ø¯ Ø§Ø­ØªÙ…Ø§Ù„ Ù†Ú©ÙˆÙ„) Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")

    # 3) Ú¯Ø§Ù… Ø³ÙˆÙ…: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø§ØªØ±ÛŒØ³ Ø²ÛŒØ§Ù†
    # ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… x_test Ø¯Ø§Ø±Ø§ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ approval_amount Ùˆ interest_amount Ø§Ø³Øª.
    cost_calc = ParsianLossMatrix(df_test=x_test, approval_col="LOAN_AMOUNT", interest_col="CURRENT_LOAN_RATES")
    cost_calc.compute_costs()
    all_costs = cost_calc.get_all_costs()

    # 4) Ú¯Ø§Ù… Ú†Ù‡Ø§Ø±Ù…: Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú†Ù†Ø¯Ù‡Ø¯ÙÙ‡ Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ NSGA-II
    from numpy import array

    # ------------------------------------------------------------------
    # 4) Ú¯Ø§Ù… Ú†Ù‡Ø§Ø±Ù…: Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú†Ù†Ø¯Ù‡Ø¯ÙÙ‡ Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ NSGAâ€‘II  (u*, v*)
    # ------------------------------------------------------------------
    threshold_nsgaii = ParsianThresholdNSGA2(
        probabilities_test=probabilities_test,
        cost_matrix=all_costs,
        true_labels=y_test.values,  # ÛŒØ§ np.array(y_test)
        pop_size=50,
        n_gen=100,
        step_bnd=False
    )
    threshold_nsgaii.optimize()

    solutions, objectives = threshold_nsgaii.get_pareto_front()
    visualizer.plot_pareto_front(objectives)

    logging.info("ğŸ”¹ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§Ø±ØªÙˆ (u,v) Ùˆ Ù…Ù‚Ø¯Ø§Ø± Ø§Ù‡Ø¯Ø§Ù (cost, boundary):")
    for (u, v), (cost_val, bnd_val) in zip(solutions, objectives):
        logging.info(f"  u={u:.3f}, v={v:.3f}  â†’  cost={cost_val:,.2f},  boundary={bnd_val:.3f}")

    # Ø§Ù†ØªØ®Ø§Ø¨ Ø¬ÙØªÙ (u*, v*) Ø¨Ø§ Ú©Ù…ØªØ±ÛŒÙ† Ø§Ù†Ø¯Ø§Ø²Ù‡Ù” Ù†Ø§Ø­ÛŒÙ‡Ù” Ù…Ø±Ø²ÛŒ
    (best_u, best_v), best_obj = threshold_nsgaii.get_final_solution()
    logging.warning(
        f"ğŸ”¹ Ø¨Ù‡ØªØ±ÛŒÙ† Ø¬ÙØª Ø¶Ø±ÛŒØ¨â€ŒÙ‡Ø§: u*={best_u:.3f}, v*={best_v:.3f}  â†’  "
        f"cost={best_obj[0]:,.2f},  boundary={best_obj[1]:.3f}"
    )

    logger.warning("11111111111111111111111111111111111111111111")
    logger.warning(f"best_u: {best_u}, best_v: {best_v}")
    logger.warning("22222222222222222222222222222222222222222222")

    visualizer.plot_with_thresholds(probabilities_test, u=best_u, v=best_v)

    visualizer.plot_default_prob_hist(probabilities_test,best_u,best_v)

    logging.info("Ú¯Ø§Ù… Ú†Ù‡Ø§Ø±Ù… (NSGAâ€‘II Ú†Ù†Ø¯Ù‡Ø¯ÙÙ‡) Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.")

    # ------------------------------------------------------------------
    # 5) Ú¯Ø§Ù… Ù¾Ù†Ø¬Ù…: Ø§Ø¹Ù…Ø§Ù„ ØªØµÙ…ÛŒÙ… Ø³Ù‡â€ŒØ±Ø§Ù‡Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² (u*, v*)
    # ------------------------------------------------------------------
    threeway = ParsianThreeWayDecision(
        probabilities_test=probabilities_test,
        cost_matrix=all_costs,
        alpha_beta_pair=(best_u, best_v)  # (u*, v*)
    )
    decisions_final = threeway.apply_three_way_decision()

    cnts = threeway.get_decision_counts()
    logging.warning(
        f"Decision counts  â†’  POS: {cnts.get(1, 0)}   NEG: {cnts.get(0, 0)}   BND: {cnts.get(-1, 0)}"
    )

    # ------------------------------------------------------------------
    # 6) Ú¯Ø§Ù… Ø´Ø´Ù…: ØªØ¹ÛŒÛŒÙ† ØªÚ©Ù„ÛŒÙ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ø­ÛŒÙ‡Ù” Ù…Ø±Ø²ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ Ú©Ù…Ú©ÛŒ
    # ------------------------------------------------------------------
    bnd_resolver = ParsianBNDResolver(
        x_train_all=x_train,
        y_train_all=y_train,
        model_type="bagging"  # ÛŒØ§ "stacking"
    )
    bnd_resolver.fit_bnd_model()

    decisions_updated = bnd_resolver.resolve_bnd_samples(x_test, decisions_final)

    logging.info("ğŸ”¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ù¾Ø³ Ø§Ø² Ú¯Ø§Ù… Ø´Ø´Ù…:")
    logging.error(
        f"   POS={np.sum(decisions_updated == 1)}, "
        f"NEG={np.sum(decisions_updated == 0)}, "
        f"BND={np.sum(decisions_updated == -1)}"
    )

    # Ø§Ø¹Ù…Ø§Ù„ Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø²ÛŒ
    decisions_updated = bnd_resolver.resolve_bnd_samples(x_test, decisions_final)

    logging.info("ğŸ”¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ù¾Ø³ Ø§Ø² Ú¯Ø§Ù… Ø´Ø´Ù…:")
    logging.error(
        f" count POS={np.sum(decisions_updated == 1)}, NEG={np.sum(decisions_updated == 0)}, BND={np.sum(decisions_updated == -1)}")

    # 7) Ú¯Ø§Ù… Ù‡ÙØªÙ…: Evaluation Ù†Ù‡Ø§ÛŒÛŒ
    final_eval = ParsianFinalEvaluator(true_labels=y_test.values, final_decisions=decisions_updated,
                                       probabilities_test=probabilities_test,  # Ø§Ú¯Ø± AUC Ø¨Ø®ÙˆØ§Ù‡ÛŒÙ…
                                       cost_matrix=all_costs  # Ø§Ú¯Ø± Ù‡Ø²ÛŒÙ†Ù‡ Ø¨Ø®ÙˆØ§Ù‡ÛŒÙ…
                                       )
    results = final_eval.evaluate_metrics()
    visualizer.plot_confusion_matrix(y_test.values, decisions_updated)

    logging.info("ğŸ”¹ Ù†ØªØ§ÛŒØ¬ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ø¯Ù„:")
    for k, v in results.items():
        logging.info(f"  {k}: {v}")

    comparator = ParsianMethodComparison(x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test,
                                         cost_matrix=all_costs,  # Ø§Ú¯Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ù‡Ø²ÛŒÙ†Ù‡ Ù‡Ù… Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´ÙˆØ¯
                                         model_comparisons=None  # Ø§Ú¯Ø± None Ø¨Ú¯Ø°Ø§Ø±ÛŒØ¯ØŒ Ú†Ù†Ø¯ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡ Ø¨Ù‡ ØµÙˆØ±Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¯Ø§Ø±Ø¯
                                         )
    comparison_df = comparator.run_comparison()
    logging.error("\nÙ†ØªØ§ÛŒØ¬ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø±Ù‚ÛŒØ¨:\n" + str(comparison_df))

    comparator.add_proposed_method_results(proposed_method_metrics=results)

    final_comparison = comparator.show_final_comparison()
    logging.info("ğŸ”¹ Ú¯Ø§Ù… Ù†Ù‡Ù… (Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ø³Ø§ÛŒØ± Ø±ÙˆØ´â€ŒÙ‡Ø§) Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
